| 05-08 13:30:52 EXPERIMENT BEGIN: 
| 05-08 13:30:52 logging into exp/qm9/edp-gnn_QM9__May-08-13-30-52_3881078/info.log
| 05-08 13:31:14 model: EdgeDensePredictionGraphScoreNetwork(
  (gnn_list): ModuleList(
    (0): EdgeDensePredictionGNNLayer(
      (multi_channel_gnn_module): GIN(
        (linear_prediction): ModuleList(
          (0): Sequential(
            (0): Linear(in_features=6, out_features=32, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=32, out_features=16, bias=True)
          )
          (1): Sequential(
            (0): Linear(in_features=16, out_features=32, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=32, out_features=16, bias=True)
          )
          (2): Sequential(
            (0): Linear(in_features=16, out_features=32, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=32, out_features=16, bias=True)
          )
          (3): Sequential(
            (0): Linear(in_features=16, out_features=32, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=32, out_features=16, bias=True)
          )
          (4): Sequential(
            (0): Linear(in_features=16, out_features=32, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=32, out_features=16, bias=True)
          )
        )
        (layers): ModuleList(
          (0): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=12, out_features=32, bias=True)
              (1): Linear(in_features=32, out_features=16, bias=True)
            )
          )
          (1): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=32, out_features=32, bias=True)
              (1): Linear(in_features=32, out_features=16, bias=True)
            )
          )
          (2): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=32, out_features=32, bias=True)
              (1): Linear(in_features=32, out_features=16, bias=True)
            )
          )
          (3): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=32, out_features=32, bias=True)
              (1): Linear(in_features=32, out_features=16, bias=True)
            )
          )
        )
      )
      (translate_mlp): MLP(
        (linears): ModuleList(
          (0): Linear(in_features=34, out_features=4, bias=True)
          (1): Linear(in_features=4, out_features=4, bias=True)
          (2): Linear(in_features=4, out_features=2, bias=True)
        )
        (batch_norms): ModuleList(
          (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (cond_layers): ModuleList(
          (0): ConditionalLayer1d()
          (1): ConditionalLayer1d()
        )
      )
    )
    (1): EdgeDensePredictionGNNLayer(
      (multi_channel_gnn_module): GIN(
        (linear_prediction): ModuleList(
          (0): Sequential(
            (0): Linear(in_features=18, out_features=36, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=36, out_features=16, bias=True)
          )
          (1): Sequential(
            (0): Linear(in_features=16, out_features=36, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=36, out_features=16, bias=True)
          )
          (2): Sequential(
            (0): Linear(in_features=16, out_features=36, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=36, out_features=16, bias=True)
          )
          (3): Sequential(
            (0): Linear(in_features=16, out_features=36, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=36, out_features=16, bias=True)
          )
          (4): Sequential(
            (0): Linear(in_features=16, out_features=36, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=36, out_features=16, bias=True)
          )
        )
        (layers): ModuleList(
          (0): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=36, out_features=36, bias=True)
              (1): Linear(in_features=36, out_features=16, bias=True)
            )
          )
          (1): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=32, out_features=36, bias=True)
              (1): Linear(in_features=36, out_features=16, bias=True)
            )
          )
          (2): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=32, out_features=36, bias=True)
              (1): Linear(in_features=36, out_features=16, bias=True)
            )
          )
          (3): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=32, out_features=36, bias=True)
              (1): Linear(in_features=36, out_features=16, bias=True)
            )
          )
        )
      )
      (translate_mlp): MLP(
        (linears): ModuleList(
          (0): Linear(in_features=34, out_features=8, bias=True)
          (1): Linear(in_features=8, out_features=8, bias=True)
          (2): Linear(in_features=8, out_features=4, bias=True)
        )
        (batch_norms): ModuleList(
          (0): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (cond_layers): ModuleList(
          (0): ConditionalLayer1d()
          (1): ConditionalLayer1d()
        )
      )
    )
    (2): EdgeDensePredictionGNNLayer(
      (multi_channel_gnn_module): GIN(
        (linear_prediction): ModuleList(
          (0): Sequential(
            (0): Linear(in_features=20, out_features=40, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=40, out_features=16, bias=True)
          )
          (1): Sequential(
            (0): Linear(in_features=16, out_features=40, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=40, out_features=16, bias=True)
          )
          (2): Sequential(
            (0): Linear(in_features=16, out_features=40, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=40, out_features=16, bias=True)
          )
          (3): Sequential(
            (0): Linear(in_features=16, out_features=40, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=40, out_features=16, bias=True)
          )
          (4): Sequential(
            (0): Linear(in_features=16, out_features=40, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=40, out_features=16, bias=True)
          )
        )
        (layers): ModuleList(
          (0): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=80, out_features=40, bias=True)
              (1): Linear(in_features=40, out_features=16, bias=True)
            )
          )
          (1): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=40, bias=True)
              (1): Linear(in_features=40, out_features=16, bias=True)
            )
          )
          (2): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=40, bias=True)
              (1): Linear(in_features=40, out_features=16, bias=True)
            )
          )
          (3): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=40, bias=True)
              (1): Linear(in_features=40, out_features=16, bias=True)
            )
          )
        )
      )
      (translate_mlp): MLP(
        (linears): ModuleList(
          (0): Linear(in_features=36, out_features=8, bias=True)
          (1): Linear(in_features=8, out_features=8, bias=True)
          (2): Linear(in_features=8, out_features=4, bias=True)
        )
        (batch_norms): ModuleList(
          (0): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (cond_layers): ModuleList(
          (0): ConditionalLayer1d()
          (1): ConditionalLayer1d()
        )
      )
    )
    (3): EdgeDensePredictionGNNLayer(
      (multi_channel_gnn_module): GIN(
        (linear_prediction): ModuleList(
          (0): Sequential(
            (0): Linear(in_features=20, out_features=40, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=40, out_features=16, bias=True)
          )
          (1): Sequential(
            (0): Linear(in_features=16, out_features=40, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=40, out_features=16, bias=True)
          )
          (2): Sequential(
            (0): Linear(in_features=16, out_features=40, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=40, out_features=16, bias=True)
          )
          (3): Sequential(
            (0): Linear(in_features=16, out_features=40, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=40, out_features=16, bias=True)
          )
          (4): Sequential(
            (0): Linear(in_features=16, out_features=40, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=40, out_features=16, bias=True)
          )
        )
        (layers): ModuleList(
          (0): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=80, out_features=40, bias=True)
              (1): Linear(in_features=40, out_features=16, bias=True)
            )
          )
          (1): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=40, bias=True)
              (1): Linear(in_features=40, out_features=16, bias=True)
            )
          )
          (2): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=40, bias=True)
              (1): Linear(in_features=40, out_features=16, bias=True)
            )
          )
          (3): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=40, bias=True)
              (1): Linear(in_features=40, out_features=16, bias=True)
            )
          )
        )
      )
      (translate_mlp): MLP(
        (linears): ModuleList(
          (0): Linear(in_features=36, out_features=8, bias=True)
          (1): Linear(in_features=8, out_features=8, bias=True)
          (2): Linear(in_features=8, out_features=4, bias=True)
        )
        (batch_norms): ModuleList(
          (0): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (cond_layers): ModuleList(
          (0): ConditionalLayer1d()
          (1): ConditionalLayer1d()
        )
      )
    )
    (4): EdgeDensePredictionGNNLayer(
      (multi_channel_gnn_module): GIN(
        (linear_prediction): ModuleList(
          (0): Sequential(
            (0): Linear(in_features=20, out_features=40, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=40, out_features=16, bias=True)
          )
          (1): Sequential(
            (0): Linear(in_features=16, out_features=40, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=40, out_features=16, bias=True)
          )
          (2): Sequential(
            (0): Linear(in_features=16, out_features=40, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=40, out_features=16, bias=True)
          )
          (3): Sequential(
            (0): Linear(in_features=16, out_features=40, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=40, out_features=16, bias=True)
          )
          (4): Sequential(
            (0): Linear(in_features=16, out_features=40, bias=True)
            (1): LeakyReLU(negative_slope=0.01)
            (2): Linear(in_features=40, out_features=16, bias=True)
          )
        )
        (layers): ModuleList(
          (0): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=80, out_features=40, bias=True)
              (1): Linear(in_features=40, out_features=16, bias=True)
            )
          )
          (1): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=40, bias=True)
              (1): Linear(in_features=40, out_features=16, bias=True)
            )
          )
          (2): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=40, bias=True)
              (1): Linear(in_features=40, out_features=16, bias=True)
            )
          )
          (3): MLP(
            (linears): ModuleList(
              (0): Linear(in_features=64, out_features=40, bias=True)
              (1): Linear(in_features=40, out_features=16, bias=True)
            )
          )
        )
      )
      (translate_mlp): MLP(
        (linears): ModuleList(
          (0): Linear(in_features=36, out_features=8, bias=True)
          (1): Linear(in_features=8, out_features=8, bias=True)
          (2): Linear(in_features=8, out_features=2, bias=True)
        )
        (batch_norms): ModuleList(
          (0): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (cond_layers): ModuleList(
          (0): ConditionalLayer1d()
          (1): ConditionalLayer1d()
        )
      )
    )
  )
  (final_read_score): MLP(
    (linears): ModuleList(
      (0): Linear(in_features=18, out_features=36, bias=True)
      (1): Linear(in_features=36, out_features=36, bias=True)
      (2): Linear(in_features=36, out_features=1, bias=True)
    )
    (cond_layers): ModuleList(
      (0): ConditionalLayer1d()
      (1): ConditionalLayer1d()
    )
  )
)
| 05-08 13:31:14 Parameters: 
gnn_list.0.multi_channel_gnn_module.eps ........................................................................ torch.Size([4])
gnn_list.0.multi_channel_gnn_module.linear_prediction.0.0.weight ........................................... torch.Size([32, 6])
gnn_list.0.multi_channel_gnn_module.linear_prediction.0.0.bias ................................................ torch.Size([32])
gnn_list.0.multi_channel_gnn_module.linear_prediction.0.2.weight .......................................... torch.Size([16, 32])
gnn_list.0.multi_channel_gnn_module.linear_prediction.0.2.bias ................................................ torch.Size([16])
gnn_list.0.multi_channel_gnn_module.linear_prediction.1.0.weight .......................................... torch.Size([32, 16])
gnn_list.0.multi_channel_gnn_module.linear_prediction.1.0.bias ................................................ torch.Size([32])
gnn_list.0.multi_channel_gnn_module.linear_prediction.1.2.weight .......................................... torch.Size([16, 32])
gnn_list.0.multi_channel_gnn_module.linear_prediction.1.2.bias ................................................ torch.Size([16])
gnn_list.0.multi_channel_gnn_module.linear_prediction.2.0.weight .......................................... torch.Size([32, 16])
gnn_list.0.multi_channel_gnn_module.linear_prediction.2.0.bias ................................................ torch.Size([32])
gnn_list.0.multi_channel_gnn_module.linear_prediction.2.2.weight .......................................... torch.Size([16, 32])
gnn_list.0.multi_channel_gnn_module.linear_prediction.2.2.bias ................................................ torch.Size([16])
gnn_list.0.multi_channel_gnn_module.linear_prediction.3.0.weight .......................................... torch.Size([32, 16])
gnn_list.0.multi_channel_gnn_module.linear_prediction.3.0.bias ................................................ torch.Size([32])
gnn_list.0.multi_channel_gnn_module.linear_prediction.3.2.weight .......................................... torch.Size([16, 32])
gnn_list.0.multi_channel_gnn_module.linear_prediction.3.2.bias ................................................ torch.Size([16])
gnn_list.0.multi_channel_gnn_module.linear_prediction.4.0.weight .......................................... torch.Size([32, 16])
gnn_list.0.multi_channel_gnn_module.linear_prediction.4.0.bias ................................................ torch.Size([32])
gnn_list.0.multi_channel_gnn_module.linear_prediction.4.2.weight .......................................... torch.Size([16, 32])
gnn_list.0.multi_channel_gnn_module.linear_prediction.4.2.bias ................................................ torch.Size([16])
gnn_list.0.multi_channel_gnn_module.layers.0.linears.0.weight ............................................. torch.Size([32, 12])
gnn_list.0.multi_channel_gnn_module.layers.0.linears.0.bias ................................................... torch.Size([32])
gnn_list.0.multi_channel_gnn_module.layers.0.linears.1.weight ............................................. torch.Size([16, 32])
gnn_list.0.multi_channel_gnn_module.layers.0.linears.1.bias ................................................... torch.Size([16])
gnn_list.0.multi_channel_gnn_module.layers.1.linears.0.weight ............................................. torch.Size([32, 32])
gnn_list.0.multi_channel_gnn_module.layers.1.linears.0.bias ................................................... torch.Size([32])
gnn_list.0.multi_channel_gnn_module.layers.1.linears.1.weight ............................................. torch.Size([16, 32])
gnn_list.0.multi_channel_gnn_module.layers.1.linears.1.bias ................................................... torch.Size([16])
gnn_list.0.multi_channel_gnn_module.layers.2.linears.0.weight ............................................. torch.Size([32, 32])
gnn_list.0.multi_channel_gnn_module.layers.2.linears.0.bias ................................................... torch.Size([32])
gnn_list.0.multi_channel_gnn_module.layers.2.linears.1.weight ............................................. torch.Size([16, 32])
gnn_list.0.multi_channel_gnn_module.layers.2.linears.1.bias ................................................... torch.Size([16])
gnn_list.0.multi_channel_gnn_module.layers.3.linears.0.weight ............................................. torch.Size([32, 32])
gnn_list.0.multi_channel_gnn_module.layers.3.linears.0.bias ................................................... torch.Size([32])
gnn_list.0.multi_channel_gnn_module.layers.3.linears.1.weight ............................................. torch.Size([16, 32])
gnn_list.0.multi_channel_gnn_module.layers.3.linears.1.bias ................................................... torch.Size([16])
gnn_list.0.translate_mlp.linears.0.weight .................................................................. torch.Size([4, 34])
gnn_list.0.translate_mlp.linears.0.bias ........................................................................ torch.Size([4])
gnn_list.0.translate_mlp.linears.1.weight ................................................................... torch.Size([4, 4])
gnn_list.0.translate_mlp.linears.1.bias ........................................................................ torch.Size([4])
gnn_list.0.translate_mlp.linears.2.weight ................................................................... torch.Size([2, 4])
gnn_list.0.translate_mlp.linears.2.bias ........................................................................ torch.Size([2])
gnn_list.0.translate_mlp.batch_norms.0.weight .................................................................. torch.Size([4])
gnn_list.0.translate_mlp.batch_norms.0.bias .................................................................... torch.Size([4])
gnn_list.0.translate_mlp.batch_norms.1.weight .................................................................. torch.Size([4])
gnn_list.0.translate_mlp.batch_norms.1.bias .................................................................... torch.Size([4])
gnn_list.0.translate_mlp.cond_layers.0.gain .............................................................. torch.Size([6, 1, 4])
gnn_list.0.translate_mlp.cond_layers.0.bias .............................................................. torch.Size([6, 1, 4])
gnn_list.0.translate_mlp.cond_layers.1.gain .............................................................. torch.Size([6, 1, 4])
gnn_list.0.translate_mlp.cond_layers.1.bias .............................................................. torch.Size([6, 1, 4])
gnn_list.1.multi_channel_gnn_module.eps ........................................................................ torch.Size([4])
gnn_list.1.multi_channel_gnn_module.linear_prediction.0.0.weight .......................................... torch.Size([36, 18])
gnn_list.1.multi_channel_gnn_module.linear_prediction.0.0.bias ................................................ torch.Size([36])
gnn_list.1.multi_channel_gnn_module.linear_prediction.0.2.weight .......................................... torch.Size([16, 36])
gnn_list.1.multi_channel_gnn_module.linear_prediction.0.2.bias ................................................ torch.Size([16])
gnn_list.1.multi_channel_gnn_module.linear_prediction.1.0.weight .......................................... torch.Size([36, 16])
gnn_list.1.multi_channel_gnn_module.linear_prediction.1.0.bias ................................................ torch.Size([36])
gnn_list.1.multi_channel_gnn_module.linear_prediction.1.2.weight .......................................... torch.Size([16, 36])
gnn_list.1.multi_channel_gnn_module.linear_prediction.1.2.bias ................................................ torch.Size([16])
gnn_list.1.multi_channel_gnn_module.linear_prediction.2.0.weight .......................................... torch.Size([36, 16])
gnn_list.1.multi_channel_gnn_module.linear_prediction.2.0.bias ................................................ torch.Size([36])
gnn_list.1.multi_channel_gnn_module.linear_prediction.2.2.weight .......................................... torch.Size([16, 36])
gnn_list.1.multi_channel_gnn_module.linear_prediction.2.2.bias ................................................ torch.Size([16])
gnn_list.1.multi_channel_gnn_module.linear_prediction.3.0.weight .......................................... torch.Size([36, 16])
gnn_list.1.multi_channel_gnn_module.linear_prediction.3.0.bias ................................................ torch.Size([36])
gnn_list.1.multi_channel_gnn_module.linear_prediction.3.2.weight .......................................... torch.Size([16, 36])
gnn_list.1.multi_channel_gnn_module.linear_prediction.3.2.bias ................................................ torch.Size([16])
gnn_list.1.multi_channel_gnn_module.linear_prediction.4.0.weight .......................................... torch.Size([36, 16])
gnn_list.1.multi_channel_gnn_module.linear_prediction.4.0.bias ................................................ torch.Size([36])
gnn_list.1.multi_channel_gnn_module.linear_prediction.4.2.weight .......................................... torch.Size([16, 36])
gnn_list.1.multi_channel_gnn_module.linear_prediction.4.2.bias ................................................ torch.Size([16])
gnn_list.1.multi_channel_gnn_module.layers.0.linears.0.weight ............................................. torch.Size([36, 36])
gnn_list.1.multi_channel_gnn_module.layers.0.linears.0.bias ................................................... torch.Size([36])
gnn_list.1.multi_channel_gnn_module.layers.0.linears.1.weight ............................................. torch.Size([16, 36])
gnn_list.1.multi_channel_gnn_module.layers.0.linears.1.bias ................................................... torch.Size([16])
gnn_list.1.multi_channel_gnn_module.layers.1.linears.0.weight ............................................. torch.Size([36, 32])
gnn_list.1.multi_channel_gnn_module.layers.1.linears.0.bias ................................................... torch.Size([36])
gnn_list.1.multi_channel_gnn_module.layers.1.linears.1.weight ............................................. torch.Size([16, 36])
gnn_list.1.multi_channel_gnn_module.layers.1.linears.1.bias ................................................... torch.Size([16])
gnn_list.1.multi_channel_gnn_module.layers.2.linears.0.weight ............................................. torch.Size([36, 32])
gnn_list.1.multi_channel_gnn_module.layers.2.linears.0.bias ................................................... torch.Size([36])
gnn_list.1.multi_channel_gnn_module.layers.2.linears.1.weight ............................................. torch.Size([16, 36])
gnn_list.1.multi_channel_gnn_module.layers.2.linears.1.bias ................................................... torch.Size([16])
gnn_list.1.multi_channel_gnn_module.layers.3.linears.0.weight ............................................. torch.Size([36, 32])
gnn_list.1.multi_channel_gnn_module.layers.3.linears.0.bias ................................................... torch.Size([36])
gnn_list.1.multi_channel_gnn_module.layers.3.linears.1.weight ............................................. torch.Size([16, 36])
gnn_list.1.multi_channel_gnn_module.layers.3.linears.1.bias ................................................... torch.Size([16])
gnn_list.1.translate_mlp.linears.0.weight .................................................................. torch.Size([8, 34])
gnn_list.1.translate_mlp.linears.0.bias ........................................................................ torch.Size([8])
gnn_list.1.translate_mlp.linears.1.weight ................................................................... torch.Size([8, 8])
gnn_list.1.translate_mlp.linears.1.bias ........................................................................ torch.Size([8])
gnn_list.1.translate_mlp.linears.2.weight ................................................................... torch.Size([4, 8])
gnn_list.1.translate_mlp.linears.2.bias ........................................................................ torch.Size([4])
gnn_list.1.translate_mlp.batch_norms.0.weight .................................................................. torch.Size([8])
gnn_list.1.translate_mlp.batch_norms.0.bias .................................................................... torch.Size([8])
gnn_list.1.translate_mlp.batch_norms.1.weight .................................................................. torch.Size([8])
gnn_list.1.translate_mlp.batch_norms.1.bias .................................................................... torch.Size([8])
gnn_list.1.translate_mlp.cond_layers.0.gain .............................................................. torch.Size([6, 1, 8])
gnn_list.1.translate_mlp.cond_layers.0.bias .............................................................. torch.Size([6, 1, 8])
gnn_list.1.translate_mlp.cond_layers.1.gain .............................................................. torch.Size([6, 1, 8])
gnn_list.1.translate_mlp.cond_layers.1.bias .............................................................. torch.Size([6, 1, 8])
gnn_list.2.multi_channel_gnn_module.eps ........................................................................ torch.Size([4])
gnn_list.2.multi_channel_gnn_module.linear_prediction.0.0.weight .......................................... torch.Size([40, 20])
gnn_list.2.multi_channel_gnn_module.linear_prediction.0.0.bias ................................................ torch.Size([40])
gnn_list.2.multi_channel_gnn_module.linear_prediction.0.2.weight .......................................... torch.Size([16, 40])
gnn_list.2.multi_channel_gnn_module.linear_prediction.0.2.bias ................................................ torch.Size([16])
gnn_list.2.multi_channel_gnn_module.linear_prediction.1.0.weight .......................................... torch.Size([40, 16])
gnn_list.2.multi_channel_gnn_module.linear_prediction.1.0.bias ................................................ torch.Size([40])
gnn_list.2.multi_channel_gnn_module.linear_prediction.1.2.weight .......................................... torch.Size([16, 40])
gnn_list.2.multi_channel_gnn_module.linear_prediction.1.2.bias ................................................ torch.Size([16])
gnn_list.2.multi_channel_gnn_module.linear_prediction.2.0.weight .......................................... torch.Size([40, 16])
gnn_list.2.multi_channel_gnn_module.linear_prediction.2.0.bias ................................................ torch.Size([40])
gnn_list.2.multi_channel_gnn_module.linear_prediction.2.2.weight .......................................... torch.Size([16, 40])
gnn_list.2.multi_channel_gnn_module.linear_prediction.2.2.bias ................................................ torch.Size([16])
gnn_list.2.multi_channel_gnn_module.linear_prediction.3.0.weight .......................................... torch.Size([40, 16])
gnn_list.2.multi_channel_gnn_module.linear_prediction.3.0.bias ................................................ torch.Size([40])
gnn_list.2.multi_channel_gnn_module.linear_prediction.3.2.weight .......................................... torch.Size([16, 40])
gnn_list.2.multi_channel_gnn_module.linear_prediction.3.2.bias ................................................ torch.Size([16])
gnn_list.2.multi_channel_gnn_module.linear_prediction.4.0.weight .......................................... torch.Size([40, 16])
gnn_list.2.multi_channel_gnn_module.linear_prediction.4.0.bias ................................................ torch.Size([40])
gnn_list.2.multi_channel_gnn_module.linear_prediction.4.2.weight .......................................... torch.Size([16, 40])
gnn_list.2.multi_channel_gnn_module.linear_prediction.4.2.bias ................................................ torch.Size([16])
gnn_list.2.multi_channel_gnn_module.layers.0.linears.0.weight ............................................. torch.Size([40, 80])
gnn_list.2.multi_channel_gnn_module.layers.0.linears.0.bias ................................................... torch.Size([40])
gnn_list.2.multi_channel_gnn_module.layers.0.linears.1.weight ............................................. torch.Size([16, 40])
gnn_list.2.multi_channel_gnn_module.layers.0.linears.1.bias ................................................... torch.Size([16])
gnn_list.2.multi_channel_gnn_module.layers.1.linears.0.weight ............................................. torch.Size([40, 64])
gnn_list.2.multi_channel_gnn_module.layers.1.linears.0.bias ................................................... torch.Size([40])
gnn_list.2.multi_channel_gnn_module.layers.1.linears.1.weight ............................................. torch.Size([16, 40])
gnn_list.2.multi_channel_gnn_module.layers.1.linears.1.bias ................................................... torch.Size([16])
gnn_list.2.multi_channel_gnn_module.layers.2.linears.0.weight ............................................. torch.Size([40, 64])
gnn_list.2.multi_channel_gnn_module.layers.2.linears.0.bias ................................................... torch.Size([40])
gnn_list.2.multi_channel_gnn_module.layers.2.linears.1.weight ............................................. torch.Size([16, 40])
gnn_list.2.multi_channel_gnn_module.layers.2.linears.1.bias ................................................... torch.Size([16])
gnn_list.2.multi_channel_gnn_module.layers.3.linears.0.weight ............................................. torch.Size([40, 64])
gnn_list.2.multi_channel_gnn_module.layers.3.linears.0.bias ................................................... torch.Size([40])
gnn_list.2.multi_channel_gnn_module.layers.3.linears.1.weight ............................................. torch.Size([16, 40])
gnn_list.2.multi_channel_gnn_module.layers.3.linears.1.bias ................................................... torch.Size([16])
gnn_list.2.translate_mlp.linears.0.weight .................................................................. torch.Size([8, 36])
gnn_list.2.translate_mlp.linears.0.bias ........................................................................ torch.Size([8])
gnn_list.2.translate_mlp.linears.1.weight ................................................................... torch.Size([8, 8])
gnn_list.2.translate_mlp.linears.1.bias ........................................................................ torch.Size([8])
gnn_list.2.translate_mlp.linears.2.weight ................................................................... torch.Size([4, 8])
gnn_list.2.translate_mlp.linears.2.bias ........................................................................ torch.Size([4])
gnn_list.2.translate_mlp.batch_norms.0.weight .................................................................. torch.Size([8])
gnn_list.2.translate_mlp.batch_norms.0.bias .................................................................... torch.Size([8])
gnn_list.2.translate_mlp.batch_norms.1.weight .................................................................. torch.Size([8])
gnn_list.2.translate_mlp.batch_norms.1.bias .................................................................... torch.Size([8])
gnn_list.2.translate_mlp.cond_layers.0.gain .............................................................. torch.Size([6, 1, 8])
gnn_list.2.translate_mlp.cond_layers.0.bias .............................................................. torch.Size([6, 1, 8])
gnn_list.2.translate_mlp.cond_layers.1.gain .............................................................. torch.Size([6, 1, 8])
gnn_list.2.translate_mlp.cond_layers.1.bias .............................................................. torch.Size([6, 1, 8])
gnn_list.3.multi_channel_gnn_module.eps ........................................................................ torch.Size([4])
gnn_list.3.multi_channel_gnn_module.linear_prediction.0.0.weight .......................................... torch.Size([40, 20])
gnn_list.3.multi_channel_gnn_module.linear_prediction.0.0.bias ................................................ torch.Size([40])
gnn_list.3.multi_channel_gnn_module.linear_prediction.0.2.weight .......................................... torch.Size([16, 40])
gnn_list.3.multi_channel_gnn_module.linear_prediction.0.2.bias ................................................ torch.Size([16])
gnn_list.3.multi_channel_gnn_module.linear_prediction.1.0.weight .......................................... torch.Size([40, 16])
gnn_list.3.multi_channel_gnn_module.linear_prediction.1.0.bias ................................................ torch.Size([40])
gnn_list.3.multi_channel_gnn_module.linear_prediction.1.2.weight .......................................... torch.Size([16, 40])
gnn_list.3.multi_channel_gnn_module.linear_prediction.1.2.bias ................................................ torch.Size([16])
gnn_list.3.multi_channel_gnn_module.linear_prediction.2.0.weight .......................................... torch.Size([40, 16])
gnn_list.3.multi_channel_gnn_module.linear_prediction.2.0.bias ................................................ torch.Size([40])
gnn_list.3.multi_channel_gnn_module.linear_prediction.2.2.weight .......................................... torch.Size([16, 40])
gnn_list.3.multi_channel_gnn_module.linear_prediction.2.2.bias ................................................ torch.Size([16])
gnn_list.3.multi_channel_gnn_module.linear_prediction.3.0.weight .......................................... torch.Size([40, 16])
gnn_list.3.multi_channel_gnn_module.linear_prediction.3.0.bias ................................................ torch.Size([40])
gnn_list.3.multi_channel_gnn_module.linear_prediction.3.2.weight .......................................... torch.Size([16, 40])
gnn_list.3.multi_channel_gnn_module.linear_prediction.3.2.bias ................................................ torch.Size([16])
gnn_list.3.multi_channel_gnn_module.linear_prediction.4.0.weight .......................................... torch.Size([40, 16])
gnn_list.3.multi_channel_gnn_module.linear_prediction.4.0.bias ................................................ torch.Size([40])
gnn_list.3.multi_channel_gnn_module.linear_prediction.4.2.weight .......................................... torch.Size([16, 40])
gnn_list.3.multi_channel_gnn_module.linear_prediction.4.2.bias ................................................ torch.Size([16])
gnn_list.3.multi_channel_gnn_module.layers.0.linears.0.weight ............................................. torch.Size([40, 80])
gnn_list.3.multi_channel_gnn_module.layers.0.linears.0.bias ................................................... torch.Size([40])
gnn_list.3.multi_channel_gnn_module.layers.0.linears.1.weight ............................................. torch.Size([16, 40])
gnn_list.3.multi_channel_gnn_module.layers.0.linears.1.bias ................................................... torch.Size([16])
gnn_list.3.multi_channel_gnn_module.layers.1.linears.0.weight ............................................. torch.Size([40, 64])
gnn_list.3.multi_channel_gnn_module.layers.1.linears.0.bias ................................................... torch.Size([40])
gnn_list.3.multi_channel_gnn_module.layers.1.linears.1.weight ............................................. torch.Size([16, 40])
gnn_list.3.multi_channel_gnn_module.layers.1.linears.1.bias ................................................... torch.Size([16])
gnn_list.3.multi_channel_gnn_module.layers.2.linears.0.weight ............................................. torch.Size([40, 64])
gnn_list.3.multi_channel_gnn_module.layers.2.linears.0.bias ................................................... torch.Size([40])
gnn_list.3.multi_channel_gnn_module.layers.2.linears.1.weight ............................................. torch.Size([16, 40])
gnn_list.3.multi_channel_gnn_module.layers.2.linears.1.bias ................................................... torch.Size([16])
gnn_list.3.multi_channel_gnn_module.layers.3.linears.0.weight ............................................. torch.Size([40, 64])
gnn_list.3.multi_channel_gnn_module.layers.3.linears.0.bias ................................................... torch.Size([40])
gnn_list.3.multi_channel_gnn_module.layers.3.linears.1.weight ............................................. torch.Size([16, 40])
gnn_list.3.multi_channel_gnn_module.layers.3.linears.1.bias ................................................... torch.Size([16])
gnn_list.3.translate_mlp.linears.0.weight .................................................................. torch.Size([8, 36])
gnn_list.3.translate_mlp.linears.0.bias ........................................................................ torch.Size([8])
gnn_list.3.translate_mlp.linears.1.weight ................................................................... torch.Size([8, 8])
gnn_list.3.translate_mlp.linears.1.bias ........................................................................ torch.Size([8])
gnn_list.3.translate_mlp.linears.2.weight ................................................................... torch.Size([4, 8])
gnn_list.3.translate_mlp.linears.2.bias ........................................................................ torch.Size([4])
gnn_list.3.translate_mlp.batch_norms.0.weight .................................................................. torch.Size([8])
gnn_list.3.translate_mlp.batch_norms.0.bias .................................................................... torch.Size([8])
gnn_list.3.translate_mlp.batch_norms.1.weight .................................................................. torch.Size([8])
gnn_list.3.translate_mlp.batch_norms.1.bias .................................................................... torch.Size([8])
gnn_list.3.translate_mlp.cond_layers.0.gain .............................................................. torch.Size([6, 1, 8])
gnn_list.3.translate_mlp.cond_layers.0.bias .............................................................. torch.Size([6, 1, 8])
gnn_list.3.translate_mlp.cond_layers.1.gain .............................................................. torch.Size([6, 1, 8])
gnn_list.3.translate_mlp.cond_layers.1.bias .............................................................. torch.Size([6, 1, 8])
gnn_list.4.multi_channel_gnn_module.eps ........................................................................ torch.Size([4])
gnn_list.4.multi_channel_gnn_module.linear_prediction.0.0.weight .......................................... torch.Size([40, 20])
gnn_list.4.multi_channel_gnn_module.linear_prediction.0.0.bias ................................................ torch.Size([40])
gnn_list.4.multi_channel_gnn_module.linear_prediction.0.2.weight .......................................... torch.Size([16, 40])
gnn_list.4.multi_channel_gnn_module.linear_prediction.0.2.bias ................................................ torch.Size([16])
gnn_list.4.multi_channel_gnn_module.linear_prediction.1.0.weight .......................................... torch.Size([40, 16])
gnn_list.4.multi_channel_gnn_module.linear_prediction.1.0.bias ................................................ torch.Size([40])
gnn_list.4.multi_channel_gnn_module.linear_prediction.1.2.weight .......................................... torch.Size([16, 40])
gnn_list.4.multi_channel_gnn_module.linear_prediction.1.2.bias ................................................ torch.Size([16])
gnn_list.4.multi_channel_gnn_module.linear_prediction.2.0.weight .......................................... torch.Size([40, 16])
gnn_list.4.multi_channel_gnn_module.linear_prediction.2.0.bias ................................................ torch.Size([40])
gnn_list.4.multi_channel_gnn_module.linear_prediction.2.2.weight .......................................... torch.Size([16, 40])
gnn_list.4.multi_channel_gnn_module.linear_prediction.2.2.bias ................................................ torch.Size([16])
gnn_list.4.multi_channel_gnn_module.linear_prediction.3.0.weight .......................................... torch.Size([40, 16])
gnn_list.4.multi_channel_gnn_module.linear_prediction.3.0.bias ................................................ torch.Size([40])
gnn_list.4.multi_channel_gnn_module.linear_prediction.3.2.weight .......................................... torch.Size([16, 40])
gnn_list.4.multi_channel_gnn_module.linear_prediction.3.2.bias ................................................ torch.Size([16])
gnn_list.4.multi_channel_gnn_module.linear_prediction.4.0.weight .......................................... torch.Size([40, 16])
gnn_list.4.multi_channel_gnn_module.linear_prediction.4.0.bias ................................................ torch.Size([40])
gnn_list.4.multi_channel_gnn_module.linear_prediction.4.2.weight .......................................... torch.Size([16, 40])
gnn_list.4.multi_channel_gnn_module.linear_prediction.4.2.bias ................................................ torch.Size([16])
gnn_list.4.multi_channel_gnn_module.layers.0.linears.0.weight ............................................. torch.Size([40, 80])
gnn_list.4.multi_channel_gnn_module.layers.0.linears.0.bias ................................................... torch.Size([40])
gnn_list.4.multi_channel_gnn_module.layers.0.linears.1.weight ............................................. torch.Size([16, 40])
gnn_list.4.multi_channel_gnn_module.layers.0.linears.1.bias ................................................... torch.Size([16])
gnn_list.4.multi_channel_gnn_module.layers.1.linears.0.weight ............................................. torch.Size([40, 64])
gnn_list.4.multi_channel_gnn_module.layers.1.linears.0.bias ................................................... torch.Size([40])
gnn_list.4.multi_channel_gnn_module.layers.1.linears.1.weight ............................................. torch.Size([16, 40])
gnn_list.4.multi_channel_gnn_module.layers.1.linears.1.bias ................................................... torch.Size([16])
gnn_list.4.multi_channel_gnn_module.layers.2.linears.0.weight ............................................. torch.Size([40, 64])
gnn_list.4.multi_channel_gnn_module.layers.2.linears.0.bias ................................................... torch.Size([40])
gnn_list.4.multi_channel_gnn_module.layers.2.linears.1.weight ............................................. torch.Size([16, 40])
gnn_list.4.multi_channel_gnn_module.layers.2.linears.1.bias ................................................... torch.Size([16])
gnn_list.4.multi_channel_gnn_module.layers.3.linears.0.weight ............................................. torch.Size([40, 64])
gnn_list.4.multi_channel_gnn_module.layers.3.linears.0.bias ................................................... torch.Size([40])
gnn_list.4.multi_channel_gnn_module.layers.3.linears.1.weight ............................................. torch.Size([16, 40])
gnn_list.4.multi_channel_gnn_module.layers.3.linears.1.bias ................................................... torch.Size([16])
gnn_list.4.translate_mlp.linears.0.weight .................................................................. torch.Size([8, 36])
gnn_list.4.translate_mlp.linears.0.bias ........................................................................ torch.Size([8])
gnn_list.4.translate_mlp.linears.1.weight ................................................................... torch.Size([8, 8])
gnn_list.4.translate_mlp.linears.1.bias ........................................................................ torch.Size([8])
gnn_list.4.translate_mlp.linears.2.weight ................................................................... torch.Size([2, 8])
gnn_list.4.translate_mlp.linears.2.bias ........................................................................ torch.Size([2])
gnn_list.4.translate_mlp.batch_norms.0.weight .................................................................. torch.Size([8])
gnn_list.4.translate_mlp.batch_norms.0.bias .................................................................... torch.Size([8])
gnn_list.4.translate_mlp.batch_norms.1.weight .................................................................. torch.Size([8])
gnn_list.4.translate_mlp.batch_norms.1.bias .................................................................... torch.Size([8])
gnn_list.4.translate_mlp.cond_layers.0.gain .............................................................. torch.Size([6, 1, 8])
gnn_list.4.translate_mlp.cond_layers.0.bias .............................................................. torch.Size([6, 1, 8])
gnn_list.4.translate_mlp.cond_layers.1.gain .............................................................. torch.Size([6, 1, 8])
gnn_list.4.translate_mlp.cond_layers.1.bias .............................................................. torch.Size([6, 1, 8])
final_read_score.linears.0.weight ......................................................................... torch.Size([36, 18])
final_read_score.linears.0.bias ............................................................................... torch.Size([36])
final_read_score.linears.1.weight ......................................................................... torch.Size([36, 36])
final_read_score.linears.1.bias ............................................................................... torch.Size([36])
final_read_score.linears.2.weight .......................................................................... torch.Size([1, 36])
final_read_score.linears.2.bias ................................................................................ torch.Size([1])
final_read_score.cond_layers.0.gain ..................................................................... torch.Size([6, 1, 36])
final_read_score.cond_layers.0.bias ..................................................................... torch.Size([6, 1, 36])
final_read_score.cond_layers.1.gain ..................................................................... torch.Size([6, 1, 36])
final_read_score.cond_layers.1.bias ..................................................................... torch.Size([6, 1, 36])
| 05-08 13:31:14 Parameters Count: 91301, Trainable: 91301
| 05-08 13:31:14 [0.1, 0.2, 0.4, 0.6, 0.8, 1.6], 0.0
| 05-08 13:32:31 epoch: 000| time: 77.6s| train loss: +7.109e+01 | test loss: +4.647e+01 | 
| 05-08 13:32:31 epoch: 000| train loss i: [25.45	18.54	10.89	 7.35	 5.71	 3.14] test loss i: [20.28	13.38	 6.63	 3.41	 2.13	 0.64] | 
| 05-08 13:33:49 epoch: 001| time: 78.1s| train loss: +4.050e+01 | test loss: +3.994e+01 | 
| 05-08 13:33:49 epoch: 001| train loss i: [16.66	12.33	 5.85	 3.13	 1.94	 0.58] test loss i: [16.43	12.19	 5.77	 3.09	 1.89	 0.56] | 
| 05-08 13:35:07 epoch: 002| time: 77.5s| train loss: +3.999e+01 | test loss: +4.038e+01 | 
| 05-08 13:35:07 epoch: 002| train loss i: [16.11	12.18	 5.86	 3.19	 1.98	 0.66] test loss i: [16.41	12.37	 5.94	 3.18	 1.88	 0.59] | 
| 05-08 13:36:24 epoch: 003| time: 77.5s| train loss: +3.856e+01 | test loss: +4.127e+01 | 
| 05-08 13:36:24 epoch: 003| train loss i: [15.2 	12.1 	 5.78	 3.08	 1.88	 0.52] test loss i: [16.61	12.73	 6.09	 3.23	 1.93	 0.69] | 
| 05-08 13:37:42 epoch: 004| time: 77.4s| train loss: +3.743e+01 | test loss: +3.920e+01 | 
| 05-08 13:37:42 epoch: 004| train loss i: [14.15	12.04	 5.78	 3.07	 1.86	 0.54] test loss i: [15.76	12.03	 5.86	 3.13	 1.89	 0.51] | 
| 05-08 13:38:59 epoch: 005| time: 77.3s| train loss: +3.664e+01 | test loss: +3.624e+01 | 
| 05-08 13:38:59 epoch: 005| train loss i: [13.36	12.03	 5.77	 3.08	 1.86	 0.54] test loss i: [12.98	12.03	 5.74	 3.09	 1.89	 0.52] | 
| 05-08 13:40:16 epoch: 006| time: 77.3s| train loss: +3.584e+01 | test loss: +3.882e+01 | 
| 05-08 13:40:16 epoch: 006| train loss i: [12.54	12.03	 5.79	 3.08	 1.88	 0.53] test loss i: [15.27	12.12	 5.83	 3.14	 1.94	 0.51] | 
| 05-08 13:41:34 epoch: 007| time: 77.2s| train loss: +3.462e+01 | test loss: +3.594e+01 | 
| 05-08 13:41:34 epoch: 007| train loss i: [11.63	11.87	 5.72	 3.05	 1.85	 0.5 ] test loss i: [12.72	12.02	 5.82	 3.03	 1.84	 0.5 ] | 
| 05-08 13:42:51 epoch: 008| time: 77.3s| train loss: +3.415e+01 | test loss: +3.788e+01 | 
| 05-08 13:42:51 epoch: 008| train loss i: [11.25	11.79	 5.72	 3.04	 1.84	 0.52] test loss i: [14.56	12.17	 5.77	 3.04	 1.83	 0.5 ] | 
| 05-08 13:44:09 epoch: 009| time: 77.7s| train loss: +3.375e+01 | test loss: +3.392e+01 | 
| 05-08 13:44:09 epoch: 009| train loss i: [10.96	11.73	 5.69	 3.03	 1.84	 0.51] test loss i: [11.  	11.8 	 5.72	 3.02	 1.84	 0.54] | 
| 05-08 13:45:26 epoch: 010| time: 77.5s| train loss: +3.348e+01 | test loss: +3.500e+01 | 
| 05-08 13:45:26 epoch: 010| train loss i: [10.75	11.68	 5.68	 3.03	 1.84	 0.51] test loss i: [11.91	11.93	 5.75	 3.06	 1.85	 0.5 ] | 
| 05-08 13:46:44 epoch: 011| time: 77.5s| train loss: +3.322e+01 | test loss: +3.507e+01 | 
| 05-08 13:46:44 epoch: 011| train loss i: [10.57	11.63	 5.67	 3.02	 1.83	 0.5 ] test loss i: [11.75	12.15	 5.66	 3.04	 1.86	 0.63] | 
| 05-08 13:48:01 epoch: 012| time: 77.8s| train loss: +3.302e+01 | test loss: +4.244e+01 | 
| 05-08 13:48:01 epoch: 012| train loss i: [10.4 	11.6 	 5.66	 3.02	 1.83	 0.51] test loss i: [17.9 	13.4 	 5.73	 3.04	 1.83	 0.54] | 
| 05-08 13:49:19 epoch: 013| time: 77.5s| train loss: +3.274e+01 | test loss: +3.369e+01 | 
| 05-08 13:49:19 epoch: 013| train loss i: [10.2 	11.57	 5.65	 3.  	 1.82	 0.5 ] test loss i: [10.93	11.83	 5.62	 3.  	 1.81	 0.49] | 
| 05-08 13:50:36 epoch: 014| time: 77.5s| train loss: +3.255e+01 | test loss: +3.466e+01 | 
| 05-08 13:50:36 epoch: 014| train loss i: [10.06	11.55	 5.64	 3.  	 1.81	 0.5 ] test loss i: [11.9 	11.81	 5.62	 3.01	 1.81	 0.49] | 
| 05-08 13:51:54 epoch: 015| time: 77.6s| train loss: +3.239e+01 | test loss: +3.268e+01 | 
| 05-08 13:51:54 epoch: 015| train loss i: [ 9.94	11.5 	 5.63	 3.  	 1.82	 0.51] test loss i: [ 9.94	11.68	 5.67	 3.03	 1.87	 0.49] | 
| 05-08 13:53:12 epoch: 016| time: 77.9s| train loss: +3.223e+01 | test loss: +3.304e+01 | 
| 05-08 13:53:12 epoch: 016| train loss i: [ 9.77	11.51	 5.63	 3.  	 1.82	 0.5 ] test loss i: [10.48	11.64	 5.59	 2.97	 1.88	 0.49] | 
| 05-08 13:54:29 epoch: 017| time: 77.4s| train loss: +3.200e+01 | test loss: +4.088e+01 | 
| 05-08 13:54:29 epoch: 017| train loss i: [ 9.63	11.45	 5.62	 2.99	 1.81	 0.5 ] test loss i: [17.26	12.7 	 5.64	 2.98	 1.8 	 0.5 ] | 
| 05-08 13:55:47 epoch: 018| time: 77.4s| train loss: +3.198e+01 | test loss: +5.824e+01 | 
| 05-08 13:55:47 epoch: 018| train loss i: [ 9.59	11.47	 5.62	 2.99	 1.81	 0.5 ] test loss i: [26.02	21.03	 5.79	 3.07	 1.82	 0.51] | 
| 05-08 13:57:05 epoch: 019| time: 77.7s| train loss: +3.186e+01 | test loss: +3.636e+01 | 
| 05-08 13:57:05 epoch: 019| train loss i: [ 9.51	11.45	 5.61	 2.99	 1.8 	 0.5 ] test loss i: [13.5 	11.89	 5.65	 2.98	 1.8 	 0.54] | 
| 05-08 13:58:22 epoch: 020| time: 77.7s| train loss: +3.173e+01 | test loss: +3.193e+01 | 
| 05-08 13:58:22 epoch: 020| train loss i: [ 9.42	11.42	 5.61	 2.99	 1.8 	 0.5 ] test loss i: [ 9.67	11.44	 5.58	 2.97	 1.79	 0.49] | 
| 05-08 13:59:40 epoch: 021| time: 77.6s| train loss: +3.153e+01 | test loss: +3.204e+01 | 
| 05-08 13:59:40 epoch: 021| train loss i: [ 9.28	11.38	 5.6 	 2.98	 1.8 	 0.49] test loss i: [ 9.77	11.39	 5.6 	 2.99	 1.8 	 0.49] | 
| 05-08 14:00:57 epoch: 022| time: 77.6s| train loss: +3.154e+01 | test loss: +3.258e+01 | 
| 05-08 14:00:57 epoch: 022| train loss i: [ 9.27	11.39	 5.6 	 2.98	 1.8 	 0.49] test loss i: [10.19	11.43	 5.68	 2.99	 1.79	 0.49] | 
| 05-08 14:02:15 epoch: 023| time: 77.6s| train loss: +3.142e+01 | test loss: +3.209e+01 | 
| 05-08 14:02:15 epoch: 023| train loss i: [ 9.18	11.36	 5.61	 2.98	 1.8 	 0.5 ] test loss i: [ 9.46	11.69	 5.64	 2.99	 1.82	 0.49] | 
| 05-08 14:03:32 epoch: 024| time: 77.4s| train loss: +3.135e+01 | test loss: +3.300e+01 | 
| 05-08 14:03:32 epoch: 024| train loss i: [ 9.13	11.34	 5.6 	 2.99	 1.8 	 0.5 ] test loss i: [10.48	11.59	 5.63	 3.02	 1.8 	 0.48] | 
| 05-08 14:04:51 epoch: 025| time: 78.2s| train loss: +3.130e+01 | test loss: +3.206e+01 | 
| 05-08 14:04:51 epoch: 025| train loss i: [ 9.1 	11.34	 5.59	 2.98	 1.8 	 0.49] test loss i: [ 9.78	11.43	 5.59	 2.97	 1.8 	 0.49] | 
| 05-08 14:06:08 epoch: 026| time: 77.5s| train loss: +3.114e+01 | test loss: +3.276e+01 | 
| 05-08 14:06:08 epoch: 026| train loss i: [ 8.93	11.33	 5.61	 2.98	 1.79	 0.5 ] test loss i: [10.28	11.49	 5.68	 3.  	 1.82	 0.48] | 
| 05-08 14:07:26 epoch: 027| time: 77.7s| train loss: +3.109e+01 | test loss: +3.210e+01 | 
| 05-08 14:07:26 epoch: 027| train loss i: [ 8.89	11.34	 5.59	 2.98	 1.79	 0.49] test loss i: [ 9.82	11.4 	 5.6 	 2.97	 1.79	 0.53] | 
| 05-08 14:08:43 epoch: 028| time: 77.6s| train loss: +3.107e+01 | test loss: +3.399e+01 | 
| 05-08 14:08:43 epoch: 028| train loss i: [ 8.9 	11.31	 5.6 	 2.98	 1.79	 0.49] test loss i: [11.41	11.62	 5.67	 3.  	 1.8 	 0.49] | 
| 05-08 14:10:01 epoch: 029| time: 77.4s| train loss: +3.097e+01 | test loss: +3.844e+01 | 
| 05-08 14:10:01 epoch: 029| train loss i: [ 8.82	11.31	 5.59	 2.98	 1.79	 0.49] test loss i: [14.66	12.66	 5.81	 3.03	 1.8 	 0.48] | 
| 05-08 14:11:18 epoch: 030| time: 77.3s| train loss: +3.096e+01 | test loss: +3.482e+01 | 
| 05-08 14:11:18 epoch: 030| train loss i: [ 8.79	11.31	 5.59	 2.98	 1.79	 0.49] test loss i: [12.28	11.57	 5.65	 3.03	 1.8 	 0.48] | 
| 05-08 14:12:36 epoch: 031| time: 78.2s| train loss: +3.088e+01 | test loss: +3.152e+01 | 
| 05-08 14:12:36 epoch: 031| train loss i: [ 8.73	11.3 	 5.59	 2.97	 1.79	 0.49] test loss i: [ 9.24	11.34	 5.66	 2.98	 1.82	 0.48] | 
| 05-08 14:13:54 epoch: 032| time: 78.1s| train loss: +3.084e+01 | test loss: +3.153e+01 | 
| 05-08 14:13:54 epoch: 032| train loss i: [ 8.72	11.28	 5.59	 2.97	 1.79	 0.49] test loss i: [ 9.19	11.44	 5.62	 3.  	 1.8 	 0.5 ] | 
| 05-08 14:15:12 epoch: 033| time: 77.9s| train loss: +3.074e+01 | test loss: +3.609e+01 | 
| 05-08 14:15:12 epoch: 033| train loss i: [ 8.63	11.27	 5.58	 2.97	 1.79	 0.49] test loss i: [13.67	11.63	 5.57	 2.96	 1.78	 0.48] | 
| 05-08 14:16:30 epoch: 034| time: 77.9s| train loss: +3.458e+01 | test loss: +3.524e+01 | 
| 05-08 14:16:30 epoch: 034| train loss i: [11.35	11.96	 5.79	 3.07	 1.87	 0.55] test loss i: [12.47	11.78	 5.65	 3.01	 1.81	 0.52] | 
| 05-08 14:17:47 epoch: 035| time: 76.8s| train loss: +3.237e+01 | test loss: +3.286e+01 | 
| 05-08 14:17:47 epoch: 035| train loss i: [ 9.93	11.52	 5.63	 2.99	 1.8 	 0.5 ] test loss i: [10.41	11.6 	 5.6 	 2.97	 1.79	 0.49] | 
| 05-08 14:19:04 epoch: 036| time: 77.0s| train loss: +3.156e+01 | test loss: +3.470e+01 | 
| 05-08 14:19:04 epoch: 036| train loss i: [ 9.23	11.45	 5.62	 2.98	 1.79	 0.49] test loss i: [12.31	11.5 	 5.6 	 2.99	 1.81	 0.49] | 
| 05-08 14:20:21 epoch: 037| time: 76.9s| train loss: +3.120e+01 | test loss: +3.192e+01 | 
| 05-08 14:20:21 epoch: 037| train loss i: [ 8.92	11.4 	 5.61	 2.98	 1.8 	 0.49] test loss i: [ 9.66	11.43	 5.59	 2.96	 1.78	 0.49] | 
| 05-08 14:21:39 epoch: 038| time: 78.1s| train loss: +3.094e+01 | test loss: +3.196e+01 | 
| 05-08 14:21:39 epoch: 038| train loss i: [ 8.74	11.36	 5.59	 2.97	 1.79	 0.49] test loss i: [ 9.52	11.45	 5.68	 3.  	 1.8 	 0.5 ] | 
| 05-08 14:22:56 epoch: 039| time: 76.9s| train loss: +3.080e+01 | test loss: +3.292e+01 | 
| 05-08 14:22:56 epoch: 039| train loss i: [ 8.63	11.33	 5.6 	 2.97	 1.79	 0.49] test loss i: [10.6 	11.48	 5.56	 2.99	 1.8 	 0.48] | 
| 05-08 14:24:14 epoch: 040| time: 77.5s| train loss: +3.073e+01 | test loss: +3.117e+01 | 
| 05-08 14:24:14 epoch: 040| train loss i: [ 8.6 	11.29	 5.59	 2.97	 1.79	 0.49] test loss i: [ 8.99	11.34	 5.62	 2.97	 1.78	 0.48] | 
| 05-08 14:25:31 epoch: 041| time: 77.6s| train loss: +3.061e+01 | test loss: +3.116e+01 | 
| 05-08 14:25:31 epoch: 041| train loss i: [ 8.51	11.27	 5.59	 2.97	 1.79	 0.49] test loss i: [ 8.89	11.39	 5.6 	 2.98	 1.81	 0.49] | 
| 05-08 14:26:48 epoch: 042| time: 77.2s| train loss: +3.047e+01 | test loss: +3.294e+01 | 
| 05-08 14:26:48 epoch: 042| train loss i: [ 8.37	11.28	 5.58	 2.97	 1.79	 0.49] test loss i: [10.4 	11.61	 5.65	 3.  	 1.8 	 0.48] | 
| 05-08 14:28:05 epoch: 043| time: 76.6s| train loss: +3.043e+01 | test loss: +3.110e+01 | 
| 05-08 14:28:05 epoch: 043| train loss i: [ 8.36	11.26	 5.57	 2.97	 1.79	 0.49] test loss i: [ 8.97	11.3 	 5.59	 2.97	 1.78	 0.48] | 
| 05-08 14:29:22 epoch: 044| time: 76.6s| train loss: +3.040e+01 | test loss: +3.545e+01 | 
| 05-08 14:29:22 epoch: 044| train loss i: [ 8.35	11.23	 5.58	 2.97	 1.79	 0.49] test loss i: [12.83	11.77	 5.6 	 2.97	 1.8 	 0.48] | 
| 05-08 14:30:38 epoch: 045| time: 76.8s| train loss: +3.031e+01 | test loss: +3.131e+01 | 
| 05-08 14:30:38 epoch: 045| train loss i: [ 8.29	11.21	 5.58	 2.96	 1.79	 0.48] test loss i: [ 8.89	11.63	 5.57	 2.96	 1.79	 0.48] | 
| 05-08 14:31:56 epoch: 046| time: 77.2s| train loss: +3.031e+01 | test loss: +3.218e+01 | 
| 05-08 14:31:56 epoch: 046| train loss i: [ 8.27	11.22	 5.58	 2.97	 1.79	 0.49] test loss i: [ 9.8 	11.48	 5.57	 3.01	 1.82	 0.49] | 
| 05-08 14:33:12 epoch: 047| time: 76.9s| train loss: +3.023e+01 | test loss: +3.394e+01 | 
| 05-08 14:33:12 epoch: 047| train loss i: [ 8.21	11.2 	 5.58	 2.96	 1.79	 0.49] test loss i: [11.18	11.77	 5.73	 3.  	 1.79	 0.48] | 
| 05-08 14:34:29 epoch: 048| time: 77.0s| train loss: +3.014e+01 | test loss: +3.184e+01 | 
| 05-08 14:34:29 epoch: 048| train loss i: [ 8.15	11.18	 5.58	 2.96	 1.78	 0.48] test loss i: [ 9.56	11.5 	 5.58	 2.94	 1.78	 0.48] | 
| 05-08 14:35:47 epoch: 049| time: 77.4s| train loss: +3.015e+01 | test loss: +3.252e+01 | 
| 05-08 14:35:47 epoch: 049| train loss i: [ 8.13	11.19	 5.58	 2.97	 1.79	 0.49] test loss i: [ 9.97	11.44	 5.76	 3.06	 1.8 	 0.49] | 
| 05-08 14:37:04 epoch: 050| time: 76.9s| train loss: +3.008e+01 | test loss: +3.054e+01 | 
| 05-08 14:37:04 epoch: 050| train loss i: [ 8.1 	11.16	 5.58	 2.96	 1.78	 0.48] test loss i: [ 8.51	11.17	 5.62	 2.97	 1.78	 0.48] | 
| 05-08 14:38:21 epoch: 051| time: 76.8s| train loss: +3.002e+01 | test loss: +3.177e+01 | 
| 05-08 14:38:21 epoch: 051| train loss i: [ 8.07	11.16	 5.56	 2.96	 1.78	 0.49] test loss i: [ 9.56	11.39	 5.58	 2.96	 1.78	 0.5 ] | 
| 05-08 14:39:37 epoch: 052| time: 76.8s| train loss: +3.004e+01 | test loss: +3.034e+01 | 
| 05-08 14:39:37 epoch: 052| train loss i: [ 8.08	11.17	 5.57	 2.96	 1.78	 0.49] test loss i: [ 8.33	11.27	 5.54	 2.94	 1.78	 0.48] | 
| 05-08 14:40:55 epoch: 053| time: 77.6s| train loss: +3.003e+01 | test loss: +3.234e+01 | 
| 05-08 14:40:55 epoch: 053| train loss i: [ 8.04	11.17	 5.57	 2.97	 1.79	 0.49] test loss i: [10.2 	11.33	 5.57	 2.97	 1.79	 0.48] | 
| 05-08 14:42:12 epoch: 054| time: 77.3s| train loss: +2.989e+01 | test loss: +3.115e+01 | 
| 05-08 14:42:12 epoch: 054| train loss i: [ 7.96	11.13	 5.57	 2.96	 1.78	 0.48] test loss i: [ 9.03	11.33	 5.59	 2.95	 1.77	 0.48] | 
| 05-08 14:43:30 epoch: 055| time: 77.3s| train loss: +2.984e+01 | test loss: +3.085e+01 | 
| 05-08 14:43:30 epoch: 055| train loss i: [ 7.9 	11.15	 5.56	 2.96	 1.78	 0.49] test loss i: [ 8.75	11.27	 5.59	 2.97	 1.77	 0.49] | 
| 05-08 14:44:48 epoch: 056| time: 78.0s| train loss: +2.987e+01 | test loss: +3.073e+01 | 
| 05-08 14:44:48 epoch: 056| train loss i: [ 7.94	11.13	 5.57	 2.96	 1.78	 0.49] test loss i: [ 8.78	11.17	 5.56	 2.96	 1.78	 0.48] | 
| 05-08 14:46:05 epoch: 057| time: 77.9s| train loss: +2.982e+01 | test loss: +3.018e+01 | 
| 05-08 14:46:05 epoch: 057| train loss i: [ 7.91	11.11	 5.56	 2.96	 1.78	 0.48] test loss i: [ 8.36	11.05	 5.55	 2.95	 1.78	 0.49] | 
| 05-08 14:47:23 epoch: 058| time: 77.7s| train loss: +2.981e+01 | test loss: +3.291e+01 | 
| 05-08 14:47:23 epoch: 058| train loss i: [ 7.91	11.1 	 5.57	 2.96	 1.78	 0.48] test loss i: [10.78	11.37	 5.55	 2.95	 1.77	 0.48] | 
| 05-08 14:48:41 epoch: 059| time: 77.5s| train loss: +2.981e+01 | test loss: +3.171e+01 | 
| 05-08 14:48:41 epoch: 059| train loss i: [ 7.9 	11.12	 5.57	 2.96	 1.78	 0.49] test loss i: [ 9.41	11.48	 5.58	 2.97	 1.79	 0.49] | 
| 05-08 14:49:59 epoch: 060| time: 78.0s| train loss: +2.976e+01 | test loss: +3.011e+01 | 
| 05-08 14:49:59 epoch: 060| train loss i: [ 7.86	11.11	 5.57	 2.96	 1.78	 0.48] test loss i: [ 8.12	11.19	 5.58	 2.97	 1.78	 0.48] | 
| 05-08 14:51:16 epoch: 061| time: 77.5s| train loss: +2.969e+01 | test loss: +3.062e+01 | 
| 05-08 14:51:16 epoch: 061| train loss i: [ 7.82	11.08	 5.56	 2.96	 1.78	 0.48] test loss i: [ 8.58	11.28	 5.56	 2.95	 1.77	 0.48] | 
| 05-08 14:52:34 epoch: 062| time: 77.6s| train loss: +2.967e+01 | test loss: +3.093e+01 | 
| 05-08 14:52:34 epoch: 062| train loss i: [ 7.8 	11.09	 5.56	 2.96	 1.78	 0.48] test loss i: [ 8.84	11.26	 5.61	 2.97	 1.78	 0.48] | 
| 05-08 14:53:51 epoch: 063| time: 77.0s| train loss: +2.966e+01 | test loss: +3.116e+01 | 
| 05-08 14:53:51 epoch: 063| train loss i: [ 7.8 	11.08	 5.56	 2.96	 1.78	 0.48] test loss i: [ 9.08	11.29	 5.56	 2.96	 1.78	 0.48] | 
| 05-08 14:55:08 epoch: 064| time: 77.1s| train loss: +2.968e+01 | test loss: +3.188e+01 | 
| 05-08 14:55:08 epoch: 064| train loss i: [ 7.81	11.09	 5.55	 2.96	 1.78	 0.48] test loss i: [ 9.61	11.36	 5.66	 2.98	 1.78	 0.48] | 
| 05-08 14:56:25 epoch: 065| time: 76.9s| train loss: +2.964e+01 | test loss: +2.980e+01 | 
| 05-08 14:56:25 epoch: 065| train loss i: [ 7.77	11.09	 5.56	 2.96	 1.78	 0.48] test loss i: [ 7.82	11.16	 5.59	 2.96	 1.78	 0.5 ] | 
| 05-08 14:57:42 epoch: 066| time: 77.1s| train loss: +2.963e+01 | test loss: +3.471e+01 | 
| 05-08 14:57:42 epoch: 066| train loss i: [ 7.76	11.09	 5.56	 2.96	 1.78	 0.48] test loss i: [12.08	11.8 	 5.58	 2.97	 1.79	 0.48] | 
| 05-08 14:58:59 epoch: 067| time: 77.1s| train loss: +2.958e+01 | test loss: +2.992e+01 | 
| 05-08 14:58:59 epoch: 067| train loss i: [ 7.74	11.06	 5.56	 2.96	 1.78	 0.48] test loss i: [ 8.05	11.08	 5.57	 2.95	 1.78	 0.48] | 
| 05-08 15:00:17 epoch: 068| time: 78.2s| train loss: +2.952e+01 | test loss: +3.039e+01 | 
| 05-08 15:00:17 epoch: 068| train loss i: [ 7.68	11.05	 5.56	 2.96	 1.78	 0.48] test loss i: [ 8.59	11.  	 5.58	 2.96	 1.78	 0.48] | 
| 05-08 15:01:34 epoch: 069| time: 77.2s| train loss: +2.952e+01 | test loss: +3.116e+01 | 
| 05-08 15:01:34 epoch: 069| train loss i: [ 7.7 	11.05	 5.56	 2.96	 1.78	 0.48] test loss i: [ 9.14	11.19	 5.6 	 2.97	 1.79	 0.48] | 
| 05-08 15:02:52 epoch: 070| time: 77.1s| train loss: +2.953e+01 | test loss: +3.147e+01 | 
| 05-08 15:02:52 epoch: 070| train loss i: [ 7.7 	11.07	 5.55	 2.95	 1.78	 0.48] test loss i: [ 9.31	11.33	 5.6 	 2.96	 1.77	 0.48] | 
| 05-08 15:04:09 epoch: 071| time: 77.3s| train loss: +2.946e+01 | test loss: +3.148e+01 | 
| 05-08 15:04:09 epoch: 071| train loss i: [ 7.65	11.03	 5.56	 2.96	 1.78	 0.48] test loss i: [ 9.45	11.27	 5.54	 2.96	 1.78	 0.48] | 
| 05-08 15:05:26 epoch: 072| time: 77.3s| train loss: +2.949e+01 | test loss: +3.190e+01 | 
| 05-08 15:05:26 epoch: 072| train loss i: [ 7.66	11.06	 5.55	 2.96	 1.78	 0.48] test loss i: [ 9.87	11.17	 5.62	 2.97	 1.78	 0.48] | 
| 05-08 15:06:43 epoch: 073| time: 77.3s| train loss: +2.939e+01 | test loss: +3.020e+01 | 
| 05-08 15:06:43 epoch: 073| train loss i: [ 7.6 	11.02	 5.56	 2.96	 1.78	 0.48] test loss i: [ 8.3 	11.16	 5.55	 2.95	 1.78	 0.48] | 
| 05-08 15:08:01 epoch: 074| time: 77.2s| train loss: +2.939e+01 | test loss: +3.016e+01 | 
| 05-08 15:08:01 epoch: 074| train loss i: [ 7.58	11.05	 5.55	 2.96	 1.78	 0.48] test loss i: [ 8.21	11.17	 5.55	 2.96	 1.78	 0.48] | 
| 05-08 15:09:18 epoch: 075| time: 77.2s| train loss: +2.940e+01 | test loss: +2.970e+01 | 
| 05-08 15:09:18 epoch: 075| train loss i: [ 7.61	11.02	 5.55	 2.96	 1.78	 0.48] test loss i: [ 7.78	11.11	 5.58	 2.96	 1.79	 0.49] | 
| 05-08 15:10:35 epoch: 076| time: 77.2s| train loss: +2.936e+01 | test loss: +3.076e+01 | 
| 05-08 15:10:35 epoch: 076| train loss i: [ 7.55	11.02	 5.56	 2.96	 1.78	 0.48] test loss i: [ 8.77	11.25	 5.53	 2.95	 1.77	 0.48] | 
| 05-08 15:11:52 epoch: 077| time: 76.8s| train loss: +2.935e+01 | test loss: +2.969e+01 | 
| 05-08 15:11:52 epoch: 077| train loss i: [ 7.56	11.02	 5.55	 2.96	 1.78	 0.48] test loss i: [ 7.73	11.08	 5.61	 2.97	 1.78	 0.51] | 
| 05-08 15:13:09 epoch: 078| time: 76.9s| train loss: +2.936e+01 | test loss: +3.018e+01 | 
| 05-08 15:13:09 epoch: 078| train loss i: [ 7.57	11.02	 5.56	 2.96	 1.78	 0.48] test loss i: [ 8.33	11.1 	 5.54	 2.96	 1.78	 0.48] | 
| 05-08 15:14:26 epoch: 079| time: 77.2s| train loss: +2.935e+01 | test loss: +3.016e+01 | 
| 05-08 15:14:26 epoch: 079| train loss i: [ 7.56	11.02	 5.56	 2.95	 1.78	 0.48] test loss i: [ 8.24	11.12	 5.59	 2.96	 1.78	 0.48] | 
| 05-08 15:15:43 epoch: 080| time: 76.8s| train loss: +2.925e+01 | test loss: +2.994e+01 | 
| 05-08 15:15:43 epoch: 080| train loss i: [ 7.5 	11.  	 5.55	 2.95	 1.78	 0.48] test loss i: [ 8.09	11.13	 5.51	 2.95	 1.78	 0.48] | 
| 05-08 15:17:00 epoch: 081| time: 76.9s| train loss: +2.932e+01 | test loss: +3.050e+01 | 
| 05-08 15:17:00 epoch: 081| train loss i: [ 7.53	11.01	 5.56	 2.96	 1.78	 0.48] test loss i: [ 8.53	11.21	 5.55	 2.94	 1.79	 0.48] | 
| 05-08 15:18:16 epoch: 082| time: 76.9s| train loss: +2.926e+01 | test loss: +3.029e+01 | 
| 05-08 15:18:16 epoch: 082| train loss i: [ 7.49	11.  	 5.55	 2.96	 1.78	 0.48] test loss i: [ 8.34	11.2 	 5.56	 2.95	 1.77	 0.48] | 
| 05-08 15:19:34 epoch: 083| time: 77.2s| train loss: +2.921e+01 | test loss: +3.031e+01 | 
| 05-08 15:19:34 epoch: 083| train loss i: [ 7.47	10.98	 5.55	 2.96	 1.78	 0.48] test loss i: [ 8.44	11.11	 5.55	 2.95	 1.78	 0.48] | 
| 05-08 15:20:51 epoch: 084| time: 77.2s| train loss: +2.922e+01 | test loss: +2.979e+01 | 
| 05-08 15:20:51 epoch: 084| train loss i: [ 7.49	10.97	 5.55	 2.96	 1.78	 0.48] test loss i: [ 7.93	11.05	 5.61	 2.96	 1.77	 0.48] | 
| 05-08 15:22:08 epoch: 085| time: 77.1s| train loss: +2.924e+01 | test loss: +2.950e+01 | 
| 05-08 15:22:08 epoch: 085| train loss i: [ 7.48	11.  	 5.55	 2.95	 1.78	 0.48] test loss i: [ 7.65	11.08	 5.56	 2.94	 1.77	 0.49] | 
| 05-08 15:23:25 epoch: 086| time: 77.5s| train loss: +2.923e+01 | test loss: +2.972e+01 | 
| 05-08 15:23:25 epoch: 086| train loss i: [ 7.49	10.98	 5.55	 2.95	 1.78	 0.48] test loss i: [ 7.86	11.1 	 5.55	 2.95	 1.77	 0.48] | 
| 05-08 15:24:42 epoch: 087| time: 77.0s| train loss: +2.922e+01 | test loss: +3.027e+01 | 
| 05-08 15:24:42 epoch: 087| train loss i: [ 7.48	10.97	 5.55	 2.95	 1.78	 0.48] test loss i: [ 8.53	11.  	 5.53	 2.95	 1.78	 0.48] | 
| 05-08 15:26:00 epoch: 088| time: 77.8s| train loss: +2.918e+01 | test loss: +2.973e+01 | 
| 05-08 15:26:00 epoch: 088| train loss i: [ 7.46	10.97	 5.55	 2.95	 1.78	 0.48] test loss i: [ 7.91	11.06	 5.56	 2.94	 1.77	 0.48] | 
| 05-08 15:27:37 epoch: 089| time: 96.4s| train loss: +2.921e+01 | test loss: +2.945e+01 | 
| 05-08 15:27:37 epoch: 089| train loss i: [ 7.49	10.97	 5.55	 2.95	 1.78	 0.48] test loss i: [ 7.73	10.98	 5.54	 2.95	 1.77	 0.48] | 
| 05-08 15:29:13 epoch: 090| time: 95.9s| train loss: +2.920e+01 | test loss: +3.019e+01 | 
| 05-08 15:29:13 epoch: 090| train loss i: [ 7.46	10.98	 5.54	 2.95	 1.78	 0.48] test loss i: [ 8.14	11.25	 5.6 	 2.94	 1.77	 0.49] | 
| 05-08 15:30:50 epoch: 091| time: 97.8s| train loss: +2.914e+01 | test loss: +2.938e+01 | 
| 05-08 15:30:50 epoch: 091| train loss i: [ 7.45	10.93	 5.54	 2.96	 1.78	 0.48] test loss i: [ 7.64	10.99	 5.54	 2.95	 1.78	 0.48] | 
| 05-08 15:32:28 epoch: 092| time: 98.0s| train loss: +2.916e+01 | test loss: +2.943e+01 | 
| 05-08 15:32:28 epoch: 092| train loss i: [ 7.44	10.96	 5.55	 2.95	 1.77	 0.48] test loss i: [ 7.69	11.  	 5.54	 2.95	 1.77	 0.48] | 
| 05-08 15:34:06 epoch: 093| time: 98.0s| train loss: +2.914e+01 | test loss: +3.001e+01 | 
| 05-08 15:34:06 epoch: 093| train loss i: [ 7.42	10.97	 5.54	 2.95	 1.78	 0.48] test loss i: [ 8.08	11.18	 5.55	 2.95	 1.77	 0.48] | 
| 05-08 15:35:44 epoch: 094| time: 97.5s| train loss: +2.909e+01 | test loss: +2.964e+01 | 
| 05-08 15:35:44 epoch: 094| train loss i: [ 7.34	10.99	 5.55	 2.96	 1.78	 0.48] test loss i: [ 7.89	11.01	 5.53	 2.95	 1.77	 0.48] | 
| 05-08 15:37:20 epoch: 095| time: 96.0s| train loss: +2.914e+01 | test loss: +2.987e+01 | 
| 05-08 15:37:20 epoch: 095| train loss i: [ 7.42	10.96	 5.54	 2.95	 1.78	 0.48] test loss i: [ 8.04	11.01	 5.58	 2.96	 1.79	 0.49] | 
| 05-08 15:38:56 epoch: 096| time: 96.3s| train loss: +2.903e+01 | test loss: +2.945e+01 | 
| 05-08 15:38:56 epoch: 096| train loss i: [ 7.36	10.93	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.68	11.  	 5.57	 2.95	 1.77	 0.48] | 
| 05-08 15:40:33 epoch: 097| time: 96.5s| train loss: +2.908e+01 | test loss: +3.024e+01 | 
| 05-08 15:40:33 epoch: 097| train loss i: [ 7.4 	10.93	 5.55	 2.95	 1.78	 0.48] test loss i: [ 8.1 	11.36	 5.58	 2.96	 1.77	 0.48] | 
| 05-08 15:42:12 epoch: 098| time: 99.1s| train loss: +2.909e+01 | test loss: +3.176e+01 | 
| 05-08 15:42:12 epoch: 098| train loss i: [ 7.36	10.97	 5.55	 2.95	 1.78	 0.48] test loss i: [ 9.72	11.28	 5.53	 2.95	 1.78	 0.49] | 
| 05-08 15:43:51 epoch: 099| time: 98.9s| train loss: +2.909e+01 | test loss: +2.934e+01 | 
| 05-08 15:43:51 epoch: 099| train loss i: [ 7.37	10.96	 5.55	 2.95	 1.78	 0.48] test loss i: [ 7.58	10.98	 5.56	 2.96	 1.78	 0.49] | 
| 05-08 15:45:27 epoch: 100| time: 96.2s| train loss: +2.903e+01 | test loss: +3.002e+01 | 
| 05-08 15:45:27 epoch: 100| train loss i: [ 7.35	10.94	 5.53	 2.95	 1.77	 0.48] test loss i: [ 8.18	11.09	 5.54	 2.95	 1.78	 0.48] | 
| 05-08 15:47:03 epoch: 101| time: 96.5s| train loss: +2.902e+01 | test loss: +2.917e+01 | 
| 05-08 15:47:03 epoch: 101| train loss i: [ 7.33	10.94	 5.54	 2.95	 1.77	 0.48] test loss i: [ 7.53	10.87	 5.54	 2.96	 1.78	 0.49] | 
| 05-08 15:48:40 epoch: 102| time: 96.5s| train loss: +2.894e+01 | test loss: +3.017e+01 | 
| 05-08 15:48:40 epoch: 102| train loss i: [ 7.26	10.94	 5.53	 2.95	 1.77	 0.48] test loss i: [ 8.33	11.05	 5.58	 2.96	 1.78	 0.48] | 
| 05-08 15:50:16 epoch: 103| time: 95.9s| train loss: +2.898e+01 | test loss: +2.906e+01 | 
| 05-08 15:50:16 epoch: 103| train loss i: [ 7.31	10.93	 5.54	 2.95	 1.78	 0.48] test loss i: [ 7.42	10.91	 5.54	 2.94	 1.77	 0.47] | 
| 05-08 15:51:52 epoch: 104| time: 95.8s| train loss: +2.892e+01 | test loss: +2.939e+01 | 
| 05-08 15:51:52 epoch: 104| train loss i: [ 7.27	10.9 	 5.55	 2.95	 1.78	 0.48] test loss i: [ 7.62	11.04	 5.52	 2.95	 1.78	 0.48] | 
| 05-08 15:53:28 epoch: 105| time: 95.9s| train loss: +2.898e+01 | test loss: +3.017e+01 | 
| 05-08 15:53:28 epoch: 105| train loss i: [ 7.3 	10.95	 5.53	 2.95	 1.78	 0.48] test loss i: [ 8.3 	11.13	 5.55	 2.95	 1.77	 0.47] | 
| 05-08 15:55:07 epoch: 106| time: 99.0s| train loss: +2.891e+01 | test loss: +3.097e+01 | 
| 05-08 15:55:07 epoch: 106| train loss i: [ 7.25	10.91	 5.54	 2.95	 1.77	 0.48] test loss i: [ 9.14	11.1 	 5.53	 2.95	 1.77	 0.47] | 
| 05-08 15:56:46 epoch: 107| time: 99.4s| train loss: +2.896e+01 | test loss: +3.026e+01 | 
| 05-08 15:56:46 epoch: 107| train loss i: [ 7.3 	10.91	 5.55	 2.95	 1.77	 0.48] test loss i: [ 8.4 	11.05	 5.56	 2.98	 1.79	 0.48] | 
| 05-08 15:58:25 epoch: 108| time: 99.1s| train loss: +2.893e+01 | test loss: +3.000e+01 | 
| 05-08 15:58:25 epoch: 108| train loss i: [ 7.28	10.91	 5.53	 2.95	 1.78	 0.48] test loss i: [ 8.18	11.13	 5.52	 2.94	 1.76	 0.48] | 
| 05-08 16:00:02 epoch: 109| time: 97.0s| train loss: +2.895e+01 | test loss: +2.908e+01 | 
| 05-08 16:00:02 epoch: 109| train loss i: [ 7.28	10.92	 5.54	 2.95	 1.78	 0.48] test loss i: [ 7.4 	10.96	 5.51	 2.94	 1.77	 0.49] | 
| 05-08 16:01:40 epoch: 110| time: 97.4s| train loss: +2.890e+01 | test loss: +2.978e+01 | 
| 05-08 16:01:40 epoch: 110| train loss i: [ 7.22	10.93	 5.55	 2.95	 1.77	 0.48] test loss i: [ 8.15	10.93	 5.52	 2.94	 1.77	 0.48] | 
| 05-08 16:03:18 epoch: 111| time: 99.0s| train loss: +2.896e+01 | test loss: +2.936e+01 | 
| 05-08 16:03:18 epoch: 111| train loss i: [ 7.29	10.93	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.69	10.94	 5.54	 2.95	 1.77	 0.47] | 
| 05-08 16:04:57 epoch: 112| time: 98.9s| train loss: +2.890e+01 | test loss: +2.922e+01 | 
| 05-08 16:04:57 epoch: 112| train loss i: [ 7.23	10.92	 5.54	 2.95	 1.77	 0.48] test loss i: [ 7.48	10.97	 5.55	 2.96	 1.77	 0.48] | 
| 05-08 16:06:36 epoch: 113| time: 98.7s| train loss: +2.894e+01 | test loss: +2.967e+01 | 
| 05-08 16:06:36 epoch: 113| train loss i: [ 7.25	10.93	 5.54	 2.95	 1.78	 0.48] test loss i: [ 7.94	11.  	 5.54	 2.95	 1.77	 0.48] | 
| 05-08 16:08:13 epoch: 114| time: 96.4s| train loss: +2.890e+01 | test loss: +2.918e+01 | 
| 05-08 16:08:13 epoch: 114| train loss i: [ 7.25	10.91	 5.54	 2.95	 1.77	 0.48] test loss i: [ 7.54	10.93	 5.52	 2.95	 1.77	 0.47] | 
| 05-08 16:09:49 epoch: 115| time: 96.6s| train loss: +2.885e+01 | test loss: +2.956e+01 | 
| 05-08 16:09:49 epoch: 115| train loss i: [ 7.22	10.89	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.76	11.04	 5.56	 2.96	 1.77	 0.48] | 
| 05-08 16:11:25 epoch: 116| time: 96.1s| train loss: +2.884e+01 | test loss: +3.007e+01 | 
| 05-08 16:11:25 epoch: 116| train loss i: [ 7.21	10.9 	 5.53	 2.95	 1.77	 0.48] test loss i: [ 8.03	11.26	 5.57	 2.96	 1.77	 0.48] | 
| 05-08 16:13:02 epoch: 117| time: 97.1s| train loss: +2.888e+01 | test loss: +2.998e+01 | 
| 05-08 16:13:02 epoch: 117| train loss i: [ 7.24	10.9 	 5.54	 2.95	 1.77	 0.48] test loss i: [ 8.23	11.01	 5.54	 2.94	 1.77	 0.48] | 
| 05-08 16:14:39 epoch: 118| time: 96.2s| train loss: +2.879e+01 | test loss: +3.048e+01 | 
| 05-08 16:14:39 epoch: 118| train loss i: [ 7.15	10.9 	 5.54	 2.95	 1.77	 0.48] test loss i: [ 8.54	11.18	 5.56	 2.95	 1.77	 0.48] | 
| 05-08 16:16:14 epoch: 119| time: 95.8s| train loss: +2.888e+01 | test loss: +2.930e+01 | 
| 05-08 16:16:14 epoch: 119| train loss i: [ 7.24	10.9 	 5.54	 2.95	 1.77	 0.48] test loss i: [ 7.5 	11.04	 5.56	 2.95	 1.78	 0.47] | 
| 05-08 16:17:51 epoch: 120| time: 96.9s| train loss: +2.884e+01 | test loss: +2.905e+01 | 
| 05-08 16:17:51 epoch: 120| train loss i: [ 7.19	10.91	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.39	10.94	 5.52	 2.94	 1.77	 0.49] | 
| 05-08 16:19:27 epoch: 121| time: 95.9s| train loss: +2.880e+01 | test loss: +2.908e+01 | 
| 05-08 16:19:27 epoch: 121| train loss i: [ 7.18	10.89	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.36	10.99	 5.54	 2.95	 1.77	 0.47] | 
| 05-08 16:21:04 epoch: 122| time: 96.7s| train loss: +2.883e+01 | test loss: +2.907e+01 | 
| 05-08 16:21:04 epoch: 122| train loss i: [ 7.21	10.89	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.48	10.87	 5.53	 2.94	 1.78	 0.47] | 
| 05-08 16:22:41 epoch: 123| time: 97.7s| train loss: +2.879e+01 | test loss: +2.895e+01 | 
| 05-08 16:22:41 epoch: 123| train loss i: [ 7.17	10.88	 5.54	 2.95	 1.78	 0.48] test loss i: [ 7.24	10.91	 5.55	 2.96	 1.78	 0.51] | 
| 05-08 16:24:21 epoch: 124| time: 99.1s| train loss: +2.880e+01 | test loss: +2.911e+01 | 
| 05-08 16:24:21 epoch: 124| train loss i: [ 7.17	10.89	 5.54	 2.95	 1.78	 0.48] test loss i: [ 7.52	10.86	 5.53	 2.95	 1.77	 0.48] | 
| 05-08 16:25:59 epoch: 125| time: 98.8s| train loss: +2.872e+01 | test loss: +2.925e+01 | 
| 05-08 16:25:59 epoch: 125| train loss i: [ 7.13	10.86	 5.53	 2.95	 1.77	 0.47] test loss i: [ 7.61	10.93	 5.51	 2.94	 1.77	 0.48] | 
| 05-08 16:27:36 epoch: 126| time: 96.9s| train loss: +2.877e+01 | test loss: +2.908e+01 | 
| 05-08 16:27:36 epoch: 126| train loss i: [ 7.15	10.88	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.46	10.89	 5.53	 2.95	 1.78	 0.48] | 
| 05-08 16:29:18 epoch: 127| time: 101.5s| train loss: +2.874e+01 | test loss: +2.886e+01 | 
| 05-08 16:29:18 epoch: 127| train loss i: [ 7.13	10.88	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.33	10.81	 5.53	 2.95	 1.77	 0.47] | 
| 05-08 16:31:04 epoch: 128| time: 106.7s| train loss: +2.869e+01 | test loss: +2.974e+01 | 
| 05-08 16:31:04 epoch: 128| train loss i: [ 7.09	10.87	 5.53	 2.95	 1.77	 0.47] test loss i: [ 8.01	11.01	 5.54	 2.93	 1.77	 0.47] | 
| 05-08 16:32:51 epoch: 129| time: 106.1s| train loss: +2.874e+01 | test loss: +2.891e+01 | 
| 05-08 16:32:51 epoch: 129| train loss i: [ 7.12	10.89	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.24	10.94	 5.53	 2.94	 1.77	 0.48] | 
| 05-08 16:34:36 epoch: 130| time: 105.6s| train loss: +2.876e+01 | test loss: +2.902e+01 | 
| 05-08 16:34:36 epoch: 130| train loss i: [ 7.14	10.89	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.41	10.86	 5.55	 2.95	 1.77	 0.47] | 
| 05-08 16:36:22 epoch: 131| time: 105.9s| train loss: +2.873e+01 | test loss: +2.892e+01 | 
| 05-08 16:36:22 epoch: 131| train loss i: [ 7.09	10.91	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.21	10.95	 5.54	 2.96	 1.78	 0.48] | 
| 05-08 16:37:59 epoch: 132| time: 97.2s| train loss: +2.870e+01 | test loss: +2.889e+01 | 
| 05-08 16:37:59 epoch: 132| train loss i: [ 7.09	10.88	 5.53	 2.95	 1.77	 0.47] test loss i: [ 7.33	10.85	 5.52	 2.95	 1.77	 0.47] | 
| 05-08 16:39:37 epoch: 133| time: 97.6s| train loss: +2.876e+01 | test loss: +2.896e+01 | 
| 05-08 16:39:37 epoch: 133| train loss i: [ 7.15	10.89	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.32	10.94	 5.52	 2.94	 1.77	 0.47] | 
| 05-08 16:41:14 epoch: 134| time: 97.0s| train loss: +2.866e+01 | test loss: +2.969e+01 | 
| 05-08 16:41:14 epoch: 134| train loss i: [ 7.07	10.87	 5.53	 2.95	 1.77	 0.47] test loss i: [ 7.83	11.13	 5.55	 2.94	 1.77	 0.47] | 
| 05-08 16:42:50 epoch: 135| time: 96.4s| train loss: +2.875e+01 | test loss: +2.903e+01 | 
| 05-08 16:42:50 epoch: 135| train loss i: [ 7.12	10.89	 5.54	 2.95	 1.77	 0.48] test loss i: [ 7.41	10.93	 5.52	 2.93	 1.77	 0.47] | 
| 05-08 16:44:27 epoch: 136| time: 96.9s| train loss: +2.871e+01 | test loss: +2.913e+01 | 
| 05-08 16:44:27 epoch: 136| train loss i: [ 7.1 	10.87	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.53	10.86	 5.54	 2.95	 1.77	 0.48] | 
| 05-08 16:46:04 epoch: 137| time: 96.4s| train loss: +2.868e+01 | test loss: +2.935e+01 | 
| 05-08 16:46:04 epoch: 137| train loss i: [ 7.09	10.87	 5.52	 2.95	 1.77	 0.48] test loss i: [ 7.76	10.9 	 5.51	 2.95	 1.76	 0.47] | 
| 05-08 16:47:40 epoch: 138| time: 96.8s| train loss: +2.868e+01 | test loss: +2.870e+01 | 
| 05-08 16:47:40 epoch: 138| train loss i: [ 7.09	10.87	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.08	10.92	 5.52	 2.94	 1.77	 0.47] | 
| 05-08 16:49:17 epoch: 139| time: 96.9s| train loss: +2.861e+01 | test loss: +2.935e+01 | 
| 05-08 16:49:17 epoch: 139| train loss i: [ 7.02	10.86	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.73	10.92	 5.53	 2.94	 1.77	 0.47] | 
| 05-08 16:50:54 epoch: 140| time: 97.0s| train loss: +2.865e+01 | test loss: +2.894e+01 | 
| 05-08 16:50:54 epoch: 140| train loss i: [ 7.07	10.85	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.25	10.96	 5.54	 2.95	 1.77	 0.47] | 
| 05-08 16:52:31 epoch: 141| time: 96.3s| train loss: +2.861e+01 | test loss: +2.892e+01 | 
| 05-08 16:52:31 epoch: 141| train loss i: [ 7.04	10.85	 5.52	 2.95	 1.77	 0.48] test loss i: [ 7.19	10.99	 5.54	 2.94	 1.77	 0.49] | 
| 05-08 16:54:07 epoch: 142| time: 96.4s| train loss: +2.869e+01 | test loss: +2.880e+01 | 
| 05-08 16:54:07 epoch: 142| train loss i: [ 7.1 	10.87	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.16	10.9 	 5.53	 2.96	 1.78	 0.47] | 
| 05-08 16:55:44 epoch: 143| time: 96.5s| train loss: +2.863e+01 | test loss: +2.893e+01 | 
| 05-08 16:55:44 epoch: 143| train loss i: [ 7.06	10.85	 5.53	 2.95	 1.77	 0.47] test loss i: [ 7.31	10.94	 5.53	 2.92	 1.76	 0.47] | 
| 05-08 16:57:20 epoch: 144| time: 96.7s| train loss: +2.857e+01 | test loss: +2.902e+01 | 
| 05-08 16:57:20 epoch: 144| train loss i: [ 7.  	10.85	 5.52	 2.95	 1.77	 0.48] test loss i: [ 7.36	10.92	 5.55	 2.95	 1.76	 0.47] | 
| 05-08 16:58:57 epoch: 145| time: 96.9s| train loss: +2.859e+01 | test loss: +2.882e+01 | 
| 05-08 16:58:57 epoch: 145| train loss i: [ 7.03	10.83	 5.53	 2.95	 1.77	 0.47] test loss i: [ 7.2 	10.93	 5.51	 2.94	 1.76	 0.48] | 
| 05-08 17:00:34 epoch: 146| time: 96.4s| train loss: +2.858e+01 | test loss: +2.920e+01 | 
| 05-08 17:00:34 epoch: 146| train loss i: [ 6.99	10.87	 5.53	 2.95	 1.77	 0.47] test loss i: [ 7.44	11.02	 5.55	 2.94	 1.77	 0.48] | 
| 05-08 17:02:10 epoch: 147| time: 96.2s| train loss: +2.864e+01 | test loss: +2.877e+01 | 
| 05-08 17:02:10 epoch: 147| train loss i: [ 7.06	10.85	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.17	10.9 	 5.52	 2.93	 1.77	 0.47] | 
| 05-08 17:03:46 epoch: 148| time: 96.2s| train loss: +2.859e+01 | test loss: +2.934e+01 | 
| 05-08 17:03:46 epoch: 148| train loss i: [ 7.02	10.85	 5.53	 2.95	 1.77	 0.47] test loss i: [ 7.6 	11.03	 5.52	 2.95	 1.77	 0.47] | 
| 05-08 17:05:22 epoch: 149| time: 96.4s| train loss: +2.860e+01 | test loss: +2.883e+01 | 
| 05-08 17:05:22 epoch: 149| train loss i: [ 7.02	10.85	 5.53	 2.95	 1.77	 0.47] test loss i: [ 7.24	10.89	 5.52	 2.94	 1.77	 0.47] | 
| 05-08 17:06:59 epoch: 150| time: 97.2s| train loss: +2.859e+01 | test loss: +2.930e+01 | 
| 05-08 17:06:59 epoch: 150| train loss i: [ 7.02	10.85	 5.53	 2.95	 1.77	 0.47] test loss i: [ 7.6 	10.95	 5.56	 2.95	 1.77	 0.47] | 
| 05-08 17:08:37 epoch: 151| time: 97.5s| train loss: +2.862e+01 | test loss: +2.870e+01 | 
| 05-08 17:08:37 epoch: 151| train loss i: [ 7.03	10.86	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.09	10.87	 5.54	 2.95	 1.78	 0.47] | 
| 05-08 17:10:09 epoch: 152| time: 92.2s| train loss: +2.857e+01 | test loss: +2.877e+01 | 
| 05-08 17:10:09 epoch: 152| train loss i: [ 7.01	10.85	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.19	10.87	 5.52	 2.95	 1.77	 0.47] | 
| 05-08 17:11:47 epoch: 153| time: 97.8s| train loss: +2.853e+01 | test loss: +2.934e+01 | 
| 05-08 17:11:47 epoch: 153| train loss i: [ 6.97	10.84	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.57	11.01	 5.55	 2.95	 1.78	 0.48] | 
| 05-08 17:13:24 epoch: 154| time: 96.8s| train loss: +2.853e+01 | test loss: +2.909e+01 | 
| 05-08 17:13:24 epoch: 154| train loss i: [ 6.99	10.82	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.43	10.93	 5.53	 2.95	 1.77	 0.47] | 
| 05-08 17:15:00 epoch: 155| time: 96.4s| train loss: +2.855e+01 | test loss: +2.937e+01 | 
| 05-08 17:15:00 epoch: 155| train loss i: [ 6.98	10.85	 5.52	 2.95	 1.77	 0.48] test loss i: [ 7.69	10.94	 5.53	 2.95	 1.78	 0.47] | 
| 05-08 17:16:37 epoch: 156| time: 96.8s| train loss: +2.854e+01 | test loss: +2.880e+01 | 
| 05-08 17:16:37 epoch: 156| train loss i: [ 6.99	10.83	 5.52	 2.95	 1.77	 0.48] test loss i: [ 7.16	10.92	 5.53	 2.94	 1.77	 0.48] | 
| 05-08 17:18:14 epoch: 157| time: 97.3s| train loss: +2.848e+01 | test loss: +2.903e+01 | 
| 05-08 17:18:14 epoch: 157| train loss i: [ 6.93	10.82	 5.53	 2.95	 1.77	 0.47] test loss i: [ 7.33	11.  	 5.5 	 2.96	 1.78	 0.47] | 
| 05-08 17:19:51 epoch: 158| time: 96.6s| train loss: +2.854e+01 | test loss: +2.902e+01 | 
| 05-08 17:19:51 epoch: 158| train loss i: [ 7.  	10.81	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.45	10.87	 5.52	 2.94	 1.77	 0.47] | 
| 05-08 17:21:27 epoch: 159| time: 96.5s| train loss: +2.852e+01 | test loss: +2.902e+01 | 
| 05-08 17:21:27 epoch: 159| train loss i: [ 6.96	10.84	 5.53	 2.95	 1.77	 0.48] test loss i: [ 7.43	10.9 	 5.5 	 2.95	 1.77	 0.47] | 
| 05-08 17:23:04 epoch: 160| time: 96.9s| train loss: +2.849e+01 | test loss: +2.886e+01 | 
| 05-08 17:23:04 epoch: 160| train loss i: [ 6.94	10.83	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.19	10.96	 5.53	 2.95	 1.77	 0.47] | 
| 05-08 17:24:42 epoch: 161| time: 97.5s| train loss: +2.849e+01 | test loss: +2.846e+01 | 
| 05-08 17:24:42 epoch: 161| train loss i: [ 6.95	10.82	 5.53	 2.95	 1.77	 0.47] test loss i: [ 6.86	10.87	 5.55	 2.94	 1.77	 0.48] | 
| 05-08 17:26:19 epoch: 162| time: 97.0s| train loss: +2.851e+01 | test loss: +2.895e+01 | 
| 05-08 17:26:19 epoch: 162| train loss i: [ 6.94	10.85	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.26	11.01	 5.51	 2.94	 1.76	 0.47] | 
| 05-08 17:27:55 epoch: 163| time: 96.7s| train loss: +2.851e+01 | test loss: +2.881e+01 | 
| 05-08 17:27:55 epoch: 163| train loss i: [ 6.97	10.83	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.13	10.94	 5.54	 2.96	 1.78	 0.47] | 
| 05-08 17:29:32 epoch: 164| time: 96.5s| train loss: +2.847e+01 | test loss: +2.848e+01 | 
| 05-08 17:29:32 epoch: 164| train loss i: [ 6.93	10.82	 5.53	 2.95	 1.77	 0.47] test loss i: [ 6.92	10.86	 5.5 	 2.95	 1.77	 0.48] | 
| 05-08 17:31:08 epoch: 165| time: 96.4s| train loss: +2.847e+01 | test loss: +2.851e+01 | 
| 05-08 17:31:08 epoch: 165| train loss i: [ 6.93	10.82	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.02	10.8 	 5.51	 2.94	 1.77	 0.47] | 
| 05-08 17:32:45 epoch: 166| time: 96.5s| train loss: +2.845e+01 | test loss: +2.894e+01 | 
| 05-08 17:32:45 epoch: 166| train loss i: [ 6.91	10.82	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.27	10.95	 5.53	 2.95	 1.77	 0.47] | 
| 05-08 17:34:21 epoch: 167| time: 96.5s| train loss: +2.843e+01 | test loss: +2.857e+01 | 
| 05-08 17:34:21 epoch: 167| train loss i: [ 6.92	10.8 	 5.51	 2.95	 1.77	 0.47] test loss i: [ 7.05	10.83	 5.51	 2.95	 1.76	 0.47] | 
| 05-08 17:35:58 epoch: 168| time: 96.7s| train loss: +2.848e+01 | test loss: +2.921e+01 | 
| 05-08 17:35:58 epoch: 168| train loss i: [ 6.94	10.83	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.6 	10.92	 5.51	 2.95	 1.77	 0.48] | 
| 05-08 17:37:34 epoch: 169| time: 96.5s| train loss: +2.844e+01 | test loss: +2.868e+01 | 
| 05-08 17:37:34 epoch: 169| train loss i: [ 6.91	10.81	 5.53	 2.95	 1.77	 0.47] test loss i: [ 7.13	10.83	 5.53	 2.94	 1.77	 0.47] | 
| 05-08 17:39:11 epoch: 170| time: 97.0s| train loss: +2.845e+01 | test loss: +2.863e+01 | 
| 05-08 17:39:11 epoch: 170| train loss i: [ 6.91	10.82	 5.53	 2.95	 1.77	 0.47] test loss i: [ 7.05	10.89	 5.5 	 2.95	 1.76	 0.47] | 
| 05-08 17:40:48 epoch: 171| time: 97.0s| train loss: +2.844e+01 | test loss: +2.883e+01 | 
| 05-08 17:40:48 epoch: 171| train loss i: [ 6.92	10.8 	 5.53	 2.95	 1.77	 0.47] test loss i: [ 7.21	10.89	 5.53	 2.95	 1.77	 0.48] | 
| 05-08 17:42:26 epoch: 172| time: 97.3s| train loss: +2.843e+01 | test loss: +2.866e+01 | 
| 05-08 17:42:26 epoch: 172| train loss i: [ 6.91	10.81	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.11	10.81	 5.55	 2.95	 1.77	 0.47] | 
| 05-08 17:44:03 epoch: 173| time: 97.0s| train loss: +2.843e+01 | test loss: +2.850e+01 | 
| 05-08 17:44:03 epoch: 173| train loss i: [ 6.89	10.82	 5.53	 2.95	 1.77	 0.47] test loss i: [ 6.95	10.79	 5.55	 2.96	 1.78	 0.47] | 
| 05-08 17:45:40 epoch: 174| time: 96.9s| train loss: +2.847e+01 | test loss: +2.891e+01 | 
| 05-08 17:45:40 epoch: 174| train loss i: [ 6.92	10.83	 5.53	 2.95	 1.77	 0.47] test loss i: [ 7.25	10.92	 5.55	 2.95	 1.78	 0.47] | 
| 05-08 17:47:16 epoch: 175| time: 96.5s| train loss: +2.848e+01 | test loss: +2.891e+01 | 
| 05-08 17:47:16 epoch: 175| train loss i: [ 6.94	10.83	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.31	10.94	 5.5 	 2.93	 1.76	 0.47] | 
| 05-08 17:48:50 epoch: 176| time: 94.1s| train loss: +2.846e+01 | test loss: +3.001e+01 | 
| 05-08 17:48:50 epoch: 176| train loss i: [ 6.92	10.83	 5.52	 2.95	 1.77	 0.47] test loss i: [ 8.22	11.02	 5.58	 2.95	 1.77	 0.47] | 
| 05-08 17:50:26 epoch: 177| time: 96.2s| train loss: +2.844e+01 | test loss: +2.890e+01 | 
| 05-08 17:50:26 epoch: 177| train loss i: [ 6.9 	10.81	 5.52	 2.95	 1.77	 0.48] test loss i: [ 7.27	10.93	 5.51	 2.94	 1.77	 0.47] | 
| 05-08 17:52:03 epoch: 178| time: 96.5s| train loss: +2.840e+01 | test loss: +2.838e+01 | 
| 05-08 17:52:03 epoch: 178| train loss i: [ 6.88	10.81	 5.53	 2.95	 1.77	 0.47] test loss i: [ 6.96	10.73	 5.51	 2.94	 1.77	 0.47] | 
| 05-08 17:53:39 epoch: 179| time: 96.4s| train loss: +2.836e+01 | test loss: +2.875e+01 | 
| 05-08 17:53:39 epoch: 179| train loss i: [ 6.84	10.8 	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.2 	10.83	 5.54	 2.94	 1.77	 0.47] | 
| 05-08 17:55:16 epoch: 180| time: 96.3s| train loss: +2.839e+01 | test loss: +2.860e+01 | 
| 05-08 17:55:16 epoch: 180| train loss i: [ 6.87	10.81	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.04	10.86	 5.51	 2.95	 1.77	 0.48] | 
| 05-08 17:56:54 epoch: 181| time: 98.2s| train loss: +2.841e+01 | test loss: +2.847e+01 | 
| 05-08 17:56:54 epoch: 181| train loss i: [ 6.89	10.81	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.88	10.88	 5.52	 2.95	 1.77	 0.47] | 
| 05-08 17:58:40 epoch: 182| time: 106.2s| train loss: +2.838e+01 | test loss: +2.872e+01 | 
| 05-08 17:58:40 epoch: 182| train loss i: [ 6.86	10.8 	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.15	10.86	 5.53	 2.94	 1.77	 0.47] | 
| 05-08 18:00:26 epoch: 183| time: 106.5s| train loss: +2.837e+01 | test loss: +2.858e+01 | 
| 05-08 18:00:26 epoch: 183| train loss i: [ 6.86	10.81	 5.51	 2.95	 1.77	 0.47] test loss i: [ 7.07	10.81	 5.52	 2.93	 1.77	 0.47] | 
| 05-08 18:02:12 epoch: 184| time: 105.7s| train loss: +2.837e+01 | test loss: +2.835e+01 | 
| 05-08 18:02:12 epoch: 184| train loss i: [ 6.86	10.79	 5.53	 2.95	 1.77	 0.47] test loss i: [ 6.85	10.8 	 5.51	 2.94	 1.77	 0.47] | 
| 05-08 18:03:57 epoch: 185| time: 105.1s| train loss: +2.839e+01 | test loss: +2.853e+01 | 
| 05-08 18:03:57 epoch: 185| train loss i: [ 6.88	10.79	 5.53	 2.95	 1.77	 0.47] test loss i: [ 6.96	10.81	 5.56	 2.95	 1.78	 0.47] | 
| 05-08 18:05:42 epoch: 186| time: 105.0s| train loss: +2.840e+01 | test loss: +2.880e+01 | 
| 05-08 18:05:42 epoch: 186| train loss i: [ 6.87	10.83	 5.51	 2.95	 1.77	 0.47] test loss i: [ 7.12	10.93	 5.55	 2.95	 1.77	 0.47] | 
| 05-08 18:07:27 epoch: 187| time: 105.0s| train loss: +2.845e+01 | test loss: +2.855e+01 | 
| 05-08 18:07:27 epoch: 187| train loss i: [ 6.91	10.83	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.97	10.92	 5.49	 2.94	 1.76	 0.47] | 
| 05-08 18:09:12 epoch: 188| time: 105.2s| train loss: +2.835e+01 | test loss: +2.860e+01 | 
| 05-08 18:09:12 epoch: 188| train loss i: [ 6.84	10.79	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.98	10.89	 5.52	 2.96	 1.78	 0.48] | 
| 05-08 18:10:57 epoch: 189| time: 104.9s| train loss: +2.833e+01 | test loss: +2.844e+01 | 
| 05-08 18:10:57 epoch: 189| train loss i: [ 6.82	10.8 	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.91	10.84	 5.52	 2.94	 1.76	 0.47] | 
| 05-08 18:12:42 epoch: 190| time: 105.1s| train loss: +2.841e+01 | test loss: +2.836e+01 | 
| 05-08 18:12:42 epoch: 190| train loss i: [ 6.9 	10.81	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.8 	10.83	 5.55	 2.95	 1.77	 0.48] | 
| 05-08 18:14:28 epoch: 191| time: 105.9s| train loss: +2.835e+01 | test loss: +2.863e+01 | 
| 05-08 18:14:28 epoch: 191| train loss i: [ 6.85	10.79	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.04	10.87	 5.53	 2.93	 1.77	 0.49] | 
| 05-08 18:16:14 epoch: 192| time: 105.5s| train loss: +2.834e+01 | test loss: +2.869e+01 | 
| 05-08 18:16:14 epoch: 192| train loss i: [ 6.84	10.79	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.04	10.9 	 5.54	 2.95	 1.78	 0.48] | 
| 05-08 18:18:00 epoch: 193| time: 105.9s| train loss: +2.831e+01 | test loss: +2.905e+01 | 
| 05-08 18:18:00 epoch: 193| train loss i: [ 6.79	10.8 	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.31	11.01	 5.55	 2.94	 1.77	 0.47] | 
| 05-08 18:19:45 epoch: 194| time: 105.4s| train loss: +2.837e+01 | test loss: +2.855e+01 | 
| 05-08 18:19:45 epoch: 194| train loss i: [ 6.85	10.81	 5.52	 2.94	 1.77	 0.47] test loss i: [ 7.01	10.82	 5.53	 2.95	 1.78	 0.48] | 
| 05-08 18:21:30 epoch: 195| time: 105.4s| train loss: +2.836e+01 | test loss: +2.838e+01 | 
| 05-08 18:21:30 epoch: 195| train loss i: [ 6.85	10.79	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.8 	10.87	 5.52	 2.94	 1.77	 0.47] | 
| 05-08 18:23:16 epoch: 196| time: 105.1s| train loss: +2.834e+01 | test loss: +2.862e+01 | 
| 05-08 18:23:16 epoch: 196| train loss i: [ 6.83	10.79	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.05	10.85	 5.54	 2.93	 1.77	 0.48] | 
| 05-08 18:25:01 epoch: 197| time: 105.0s| train loss: +2.834e+01 | test loss: +2.855e+01 | 
| 05-08 18:25:01 epoch: 197| train loss i: [ 6.83	10.79	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.  	10.86	 5.49	 2.95	 1.77	 0.47] | 
| 05-08 18:26:46 epoch: 198| time: 105.1s| train loss: +2.834e+01 | test loss: +2.859e+01 | 
| 05-08 18:26:46 epoch: 198| train loss i: [ 6.83	10.8 	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.05	10.84	 5.52	 2.95	 1.77	 0.47] | 
| 05-08 18:28:32 epoch: 199| time: 106.1s| train loss: +2.832e+01 | test loss: +2.829e+01 | 
| 05-08 18:28:32 epoch: 199| train loss i: [ 6.84	10.78	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.81	10.77	 5.52	 2.94	 1.77	 0.47] | 
| 05-08 18:30:17 epoch: 200| time: 105.6s| train loss: +2.831e+01 | test loss: +2.864e+01 | 
| 05-08 18:30:17 epoch: 200| train loss i: [ 6.81	10.79	 5.52	 2.94	 1.77	 0.47] test loss i: [ 7.14	10.8 	 5.52	 2.94	 1.77	 0.47] | 
| 05-08 18:32:02 epoch: 201| time: 105.1s| train loss: +2.831e+01 | test loss: +2.827e+01 | 
| 05-08 18:32:02 epoch: 201| train loss i: [ 6.83	10.78	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.77	10.79	 5.53	 2.94	 1.77	 0.47] | 
| 05-08 18:33:47 epoch: 202| time: 105.0s| train loss: +2.831e+01 | test loss: +2.856e+01 | 
| 05-08 18:33:47 epoch: 202| train loss i: [ 6.8 	10.8 	 5.52	 2.94	 1.77	 0.47] test loss i: [ 7.04	10.82	 5.51	 2.94	 1.77	 0.48] | 
| 05-08 18:35:33 epoch: 203| time: 105.6s| train loss: +2.825e+01 | test loss: +2.848e+01 | 
| 05-08 18:35:33 epoch: 203| train loss i: [ 6.74	10.79	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.94	10.84	 5.52	 2.95	 1.77	 0.47] | 
| 05-08 18:37:19 epoch: 204| time: 106.0s| train loss: +2.825e+01 | test loss: +2.841e+01 | 
| 05-08 18:37:19 epoch: 204| train loss i: [ 6.76	10.79	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.89	10.82	 5.51	 2.95	 1.77	 0.47] | 
| 05-08 18:39:06 epoch: 205| time: 107.4s| train loss: +2.828e+01 | test loss: +2.853e+01 | 
| 05-08 18:39:06 epoch: 205| train loss i: [ 6.8 	10.78	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.91	10.91	 5.53	 2.94	 1.77	 0.47] | 
| 05-08 18:40:54 epoch: 206| time: 107.5s| train loss: +2.828e+01 | test loss: +2.854e+01 | 
| 05-08 18:40:54 epoch: 206| train loss i: [ 6.78	10.79	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.95	10.89	 5.51	 2.94	 1.77	 0.47] | 
| 05-08 18:42:37 epoch: 207| time: 103.4s| train loss: +2.829e+01 | test loss: +2.857e+01 | 
| 05-08 18:42:37 epoch: 207| train loss i: [ 6.79	10.79	 5.51	 2.94	 1.77	 0.47] test loss i: [ 7.12	10.79	 5.49	 2.93	 1.76	 0.47] | 
| 05-08 18:44:15 epoch: 208| time: 97.8s| train loss: +2.828e+01 | test loss: +2.822e+01 | 
| 05-08 18:44:15 epoch: 208| train loss i: [ 6.79	10.77	 5.53	 2.95	 1.77	 0.47] test loss i: [ 6.76	10.78	 5.5 	 2.94	 1.77	 0.47] | 
| 05-08 18:45:52 epoch: 209| time: 97.2s| train loss: +2.827e+01 | test loss: +2.875e+01 | 
| 05-08 18:45:52 epoch: 209| train loss i: [ 6.77	10.78	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.19	10.9 	 5.48	 2.93	 1.77	 0.48] | 
| 05-08 18:47:30 epoch: 210| time: 97.3s| train loss: +2.823e+01 | test loss: +2.848e+01 | 
| 05-08 18:47:30 epoch: 210| train loss i: [ 6.75	10.78	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.99	10.8 	 5.51	 2.94	 1.77	 0.47] | 
| 05-08 18:49:04 epoch: 211| time: 94.5s| train loss: +2.830e+01 | test loss: +2.852e+01 | 
| 05-08 18:49:04 epoch: 211| train loss i: [ 6.82	10.78	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.94	10.9 	 5.52	 2.93	 1.76	 0.47] | 
| 05-08 18:50:42 epoch: 212| time: 98.0s| train loss: +2.829e+01 | test loss: +2.854e+01 | 
| 05-08 18:50:42 epoch: 212| train loss i: [ 6.79	10.79	 5.51	 2.95	 1.77	 0.47] test loss i: [ 7.04	10.8 	 5.52	 2.94	 1.77	 0.47] | 
| 05-08 18:52:20 epoch: 213| time: 98.3s| train loss: +2.828e+01 | test loss: +2.859e+01 | 
| 05-08 18:52:20 epoch: 213| train loss i: [ 6.79	10.78	 5.52	 2.94	 1.77	 0.47] test loss i: [ 7.08	10.82	 5.5 	 2.95	 1.77	 0.47] | 
| 05-08 18:53:57 epoch: 214| time: 96.9s| train loss: +2.829e+01 | test loss: +2.843e+01 | 
| 05-08 18:53:57 epoch: 214| train loss i: [ 6.8 	10.79	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.97	10.75	 5.54	 2.94	 1.76	 0.47] | 
| 05-08 18:55:35 epoch: 215| time: 98.0s| train loss: +2.826e+01 | test loss: +2.835e+01 | 
| 05-08 18:55:35 epoch: 215| train loss i: [ 6.78	10.78	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.83	10.79	 5.54	 2.95	 1.77	 0.48] | 
| 05-08 18:57:13 epoch: 216| time: 97.6s| train loss: +2.826e+01 | test loss: +2.844e+01 | 
| 05-08 18:57:13 epoch: 216| train loss i: [ 6.77	10.78	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.9 	10.84	 5.51	 2.94	 1.77	 0.49] | 
| 05-08 18:58:50 epoch: 217| time: 97.3s| train loss: +2.827e+01 | test loss: +2.838e+01 | 
| 05-08 18:58:50 epoch: 217| train loss i: [ 6.78	10.78	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.84	10.85	 5.49	 2.95	 1.77	 0.47] | 
| 05-08 19:00:28 epoch: 218| time: 97.5s| train loss: +2.823e+01 | test loss: +2.848e+01 | 
| 05-08 19:00:28 epoch: 218| train loss i: [ 6.74	10.78	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.96	10.81	 5.52	 2.94	 1.77	 0.48] | 
| 05-08 19:02:06 epoch: 219| time: 97.7s| train loss: +2.831e+01 | test loss: +2.840e+01 | 
| 05-08 19:02:06 epoch: 219| train loss i: [ 6.81	10.78	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.85	10.84	 5.52	 2.93	 1.76	 0.49] | 
| 05-08 19:03:43 epoch: 220| time: 97.5s| train loss: +2.824e+01 | test loss: +2.855e+01 | 
| 05-08 19:03:43 epoch: 220| train loss i: [ 6.76	10.78	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.96	10.91	 5.52	 2.93	 1.76	 0.47] | 
| 05-08 19:05:19 epoch: 221| time: 95.9s| train loss: +2.822e+01 | test loss: +2.840e+01 | 
| 05-08 19:05:19 epoch: 221| train loss i: [ 6.72	10.78	 5.53	 2.95	 1.77	 0.47] test loss i: [ 6.9 	10.82	 5.5 	 2.94	 1.77	 0.48] | 
| 05-08 19:06:55 epoch: 222| time: 96.1s| train loss: +2.822e+01 | test loss: +2.815e+01 | 
| 05-08 19:06:55 epoch: 222| train loss i: [ 6.77	10.75	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.75	10.72	 5.51	 2.94	 1.77	 0.47] | 
| 05-08 19:08:31 epoch: 223| time: 95.8s| train loss: +2.825e+01 | test loss: +2.841e+01 | 
| 05-08 19:08:31 epoch: 223| train loss i: [ 6.78	10.77	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.87	10.81	 5.53	 2.95	 1.77	 0.47] | 
| 05-08 19:10:07 epoch: 224| time: 96.4s| train loss: +2.823e+01 | test loss: +2.836e+01 | 
| 05-08 19:10:07 epoch: 224| train loss i: [ 6.73	10.79	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.77	10.87	 5.53	 2.95	 1.77	 0.47] | 
| 05-08 19:11:44 epoch: 225| time: 96.9s| train loss: +2.821e+01 | test loss: +2.821e+01 | 
| 05-08 19:11:44 epoch: 225| train loss i: [ 6.75	10.76	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.81	10.69	 5.52	 2.95	 1.77	 0.47] | 
| 05-08 19:13:22 epoch: 226| time: 97.4s| train loss: +2.826e+01 | test loss: +2.822e+01 | 
| 05-08 19:13:22 epoch: 226| train loss i: [ 6.78	10.77	 5.51	 2.94	 1.77	 0.48] test loss i: [ 6.8 	10.73	 5.51	 2.94	 1.77	 0.48] | 
| 05-08 19:14:59 epoch: 227| time: 97.7s| train loss: +2.826e+01 | test loss: +2.853e+01 | 
| 05-08 19:14:59 epoch: 227| train loss i: [ 6.77	10.77	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.87	10.97	 5.5 	 2.94	 1.77	 0.48] | 
| 05-08 19:16:37 epoch: 228| time: 98.1s| train loss: +2.819e+01 | test loss: +2.834e+01 | 
| 05-08 19:16:37 epoch: 228| train loss i: [ 6.72	10.77	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.86	10.78	 5.53	 2.94	 1.77	 0.47] | 
| 05-08 19:18:19 epoch: 229| time: 101.5s| train loss: +2.817e+01 | test loss: +2.853e+01 | 
| 05-08 19:18:19 epoch: 229| train loss i: [ 6.71	10.75	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.11	10.77	 5.48	 2.94	 1.76	 0.47] | 
| 05-08 19:20:01 epoch: 230| time: 102.1s| train loss: +2.823e+01 | test loss: +2.837e+01 | 
| 05-08 19:20:01 epoch: 230| train loss i: [ 6.75	10.77	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.81	10.83	 5.53	 2.95	 1.77	 0.47] | 
| 05-08 19:21:42 epoch: 231| time: 101.4s| train loss: +2.822e+01 | test loss: +2.856e+01 | 
| 05-08 19:21:42 epoch: 231| train loss i: [ 6.73	10.78	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.02	10.83	 5.53	 2.95	 1.76	 0.47] | 
| 05-08 19:23:26 epoch: 232| time: 103.3s| train loss: +2.822e+01 | test loss: +2.865e+01 | 
| 05-08 19:23:26 epoch: 232| train loss i: [ 6.76	10.75	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.1 	10.84	 5.53	 2.94	 1.77	 0.47] | 
| 05-08 19:25:07 epoch: 233| time: 101.8s| train loss: +2.823e+01 | test loss: +2.836e+01 | 
| 05-08 19:25:07 epoch: 233| train loss i: [ 6.79	10.74	 5.51	 2.94	 1.77	 0.47] test loss i: [ 6.9 	10.74	 5.52	 2.95	 1.77	 0.47] | 
| 05-08 19:26:42 epoch: 234| time: 94.9s| train loss: +2.815e+01 | test loss: +2.867e+01 | 
| 05-08 19:26:42 epoch: 234| train loss i: [ 6.7 	10.75	 5.51	 2.94	 1.77	 0.47] test loss i: [ 7.12	10.87	 5.5 	 2.95	 1.76	 0.47] | 
| 05-08 19:28:14 epoch: 235| time: 91.3s| train loss: +2.816e+01 | test loss: +2.827e+01 | 
| 05-08 19:28:14 epoch: 235| train loss i: [ 6.69	10.76	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.83	10.75	 5.51	 2.94	 1.77	 0.47] | 
| 05-08 19:29:45 epoch: 236| time: 91.9s| train loss: +2.819e+01 | test loss: +2.837e+01 | 
| 05-08 19:29:45 epoch: 236| train loss i: [ 6.72	10.76	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.85	10.79	 5.55	 2.94	 1.77	 0.47] | 
| 05-08 19:31:17 epoch: 237| time: 91.4s| train loss: +2.816e+01 | test loss: +2.836e+01 | 
| 05-08 19:31:17 epoch: 237| train loss i: [ 6.71	10.74	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.85	10.78	 5.54	 2.95	 1.78	 0.47] | 
| 05-08 19:32:49 epoch: 238| time: 92.0s| train loss: +2.818e+01 | test loss: +2.833e+01 | 
| 05-08 19:32:49 epoch: 238| train loss i: [ 6.7 	10.76	 5.53	 2.95	 1.77	 0.47] test loss i: [ 6.84	10.82	 5.49	 2.94	 1.77	 0.47] | 
| 05-08 19:34:21 epoch: 239| time: 91.9s| train loss: +2.818e+01 | test loss: +2.849e+01 | 
| 05-08 19:34:21 epoch: 239| train loss i: [ 6.72	10.75	 5.52	 2.95	 1.77	 0.47] test loss i: [ 7.  	10.8 	 5.51	 2.95	 1.77	 0.47] | 
| 05-08 19:35:53 epoch: 240| time: 91.8s| train loss: +2.812e+01 | test loss: +2.823e+01 | 
| 05-08 19:35:53 epoch: 240| train loss i: [ 6.66	10.76	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.74	10.79	 5.52	 2.93	 1.77	 0.47] | 
| 05-08 19:37:24 epoch: 241| time: 91.8s| train loss: +2.817e+01 | test loss: +2.820e+01 | 
| 05-08 19:37:24 epoch: 241| train loss i: [ 6.71	10.75	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.75	10.8 	 5.48	 2.93	 1.76	 0.47] | 
| 05-08 19:38:55 epoch: 242| time: 90.7s| train loss: +2.812e+01 | test loss: +2.831e+01 | 
| 05-08 19:38:55 epoch: 242| train loss i: [ 6.66	10.75	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.86	10.75	 5.5 	 2.95	 1.77	 0.47] | 
| 05-08 19:40:26 epoch: 243| time: 91.1s| train loss: +2.816e+01 | test loss: +2.840e+01 | 
| 05-08 19:40:26 epoch: 243| train loss i: [ 6.68	10.77	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.87	10.85	 5.5 	 2.94	 1.76	 0.48] | 
| 05-08 19:41:58 epoch: 244| time: 91.9s| train loss: +2.818e+01 | test loss: +2.845e+01 | 
| 05-08 19:41:58 epoch: 244| train loss i: [ 6.7 	10.78	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.98	10.77	 5.52	 2.94	 1.77	 0.47] | 
| 05-08 19:43:30 epoch: 245| time: 92.0s| train loss: +2.818e+01 | test loss: +2.857e+01 | 
| 05-08 19:43:30 epoch: 245| train loss i: [ 6.7 	10.77	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.97	10.84	 5.54	 2.96	 1.78	 0.48] | 
| 05-08 19:45:02 epoch: 246| time: 91.7s| train loss: +2.815e+01 | test loss: +2.826e+01 | 
| 05-08 19:45:02 epoch: 246| train loss i: [ 6.71	10.74	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.77	10.8 	 5.51	 2.94	 1.77	 0.47] | 
| 05-08 19:46:33 epoch: 247| time: 90.9s| train loss: +2.814e+01 | test loss: +2.840e+01 | 
| 05-08 19:46:33 epoch: 247| train loss i: [ 6.68	10.76	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.88	10.83	 5.5 	 2.94	 1.77	 0.47] | 
| 05-08 19:48:04 epoch: 248| time: 91.4s| train loss: +2.811e+01 | test loss: +2.837e+01 | 
| 05-08 19:48:04 epoch: 248| train loss i: [ 6.67	10.74	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.9 	10.77	 5.5 	 2.94	 1.78	 0.48] | 
| 05-08 19:49:35 epoch: 249| time: 91.4s| train loss: +2.818e+01 | test loss: +2.857e+01 | 
| 05-08 19:49:35 epoch: 249| train loss i: [ 6.71	10.76	 5.52	 2.94	 1.77	 0.47] test loss i: [ 7.  	10.86	 5.53	 2.93	 1.77	 0.47] | 
| 05-08 19:51:06 epoch: 250| time: 90.9s| train loss: +2.816e+01 | test loss: +2.813e+01 | 
| 05-08 19:51:06 epoch: 250| train loss i: [ 6.69	10.77	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.66	10.74	 5.53	 2.95	 1.77	 0.48] | 
| 05-08 19:52:38 epoch: 251| time: 92.1s| train loss: +2.814e+01 | test loss: +2.839e+01 | 
| 05-08 19:52:38 epoch: 251| train loss i: [ 6.67	10.77	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.92	10.76	 5.51	 2.94	 1.78	 0.49] | 
| 05-08 19:54:10 epoch: 252| time: 91.6s| train loss: +2.813e+01 | test loss: +2.836e+01 | 
| 05-08 19:54:10 epoch: 252| train loss i: [ 6.67	10.76	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.83	10.81	 5.53	 2.94	 1.77	 0.47] | 
| 05-08 19:55:41 epoch: 253| time: 91.2s| train loss: +2.814e+01 | test loss: +2.835e+01 | 
| 05-08 19:55:41 epoch: 253| train loss i: [ 6.67	10.76	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.93	10.72	 5.52	 2.95	 1.76	 0.47] | 
| 05-08 19:57:13 epoch: 254| time: 91.6s| train loss: +2.810e+01 | test loss: +2.815e+01 | 
| 05-08 19:57:13 epoch: 254| train loss i: [ 6.66	10.75	 5.51	 2.94	 1.77	 0.47] test loss i: [ 6.67	10.77	 5.52	 2.96	 1.77	 0.47] | 
| 05-08 19:58:45 epoch: 255| time: 91.9s| train loss: +2.812e+01 | test loss: +2.815e+01 | 
| 05-08 19:58:45 epoch: 255| train loss i: [ 6.66	10.75	 5.51	 2.94	 1.77	 0.47] test loss i: [ 6.76	10.74	 5.49	 2.93	 1.76	 0.47] | 
| 05-08 20:00:16 epoch: 256| time: 91.7s| train loss: +2.818e+01 | test loss: +2.831e+01 | 
| 05-08 20:00:16 epoch: 256| train loss i: [ 6.74	10.74	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.77	10.87	 5.5 	 2.94	 1.76	 0.47] | 
| 05-08 20:01:48 epoch: 257| time: 91.5s| train loss: +2.815e+01 | test loss: +2.850e+01 | 
| 05-08 20:01:48 epoch: 257| train loss i: [ 6.69	10.74	 5.53	 2.95	 1.77	 0.47] test loss i: [ 6.95	10.87	 5.51	 2.94	 1.76	 0.47] | 
| 05-08 20:03:19 epoch: 258| time: 91.3s| train loss: +2.812e+01 | test loss: +2.825e+01 | 
| 05-08 20:03:19 epoch: 258| train loss i: [ 6.67	10.75	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.78	10.77	 5.53	 2.94	 1.77	 0.47] | 
| 05-08 20:04:51 epoch: 259| time: 91.5s| train loss: +2.813e+01 | test loss: +2.817e+01 | 
| 05-08 20:04:51 epoch: 259| train loss i: [ 6.69	10.74	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.75	10.73	 5.51	 2.95	 1.77	 0.47] | 
| 05-08 20:06:23 epoch: 260| time: 92.2s| train loss: +2.810e+01 | test loss: +2.835e+01 | 
| 05-08 20:06:23 epoch: 260| train loss i: [ 6.66	10.74	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.82	10.81	 5.53	 2.94	 1.77	 0.48] | 
| 05-08 20:08:02 epoch: 261| time: 99.5s| train loss: +2.810e+01 | test loss: +2.835e+01 | 
| 05-08 20:08:02 epoch: 261| train loss i: [ 6.66	10.75	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.78	10.84	 5.55	 2.95	 1.77	 0.47] | 
| 05-08 20:09:41 epoch: 262| time: 98.2s| train loss: +2.812e+01 | test loss: +2.835e+01 | 
| 05-08 20:09:41 epoch: 262| train loss i: [ 6.64	10.77	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.79	10.85	 5.53	 2.94	 1.77	 0.47] | 
| 05-08 20:11:19 epoch: 263| time: 97.9s| train loss: +2.833e+01 | test loss: +2.961e+01 | 
| 05-08 20:11:19 epoch: 263| train loss i: [ 6.81	10.77	 5.53	 2.95	 1.78	 0.49] test loss i: [ 7.56	11.12	 5.6 	 3.03	 1.82	 0.48] | 
| 05-08 20:12:56 epoch: 264| time: 97.6s| train loss: +2.818e+01 | test loss: +2.814e+01 | 
| 05-08 20:12:56 epoch: 264| train loss i: [ 6.69	10.76	 5.53	 2.95	 1.77	 0.48] test loss i: [ 6.66	10.78	 5.54	 2.94	 1.76	 0.47] | 
| 05-08 20:14:31 epoch: 265| time: 95.0s| train loss: +2.811e+01 | test loss: +2.806e+01 | 
| 05-08 20:14:31 epoch: 265| train loss i: [ 6.66	10.74	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.7 	10.69	 5.51	 2.93	 1.76	 0.47] | 
| 05-08 20:16:03 epoch: 266| time: 92.1s| train loss: +2.812e+01 | test loss: +2.833e+01 | 
| 05-08 20:16:03 epoch: 266| train loss i: [ 6.67	10.74	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.8 	10.83	 5.52	 2.94	 1.77	 0.47] | 
| 05-08 20:17:36 epoch: 267| time: 92.4s| train loss: +2.814e+01 | test loss: +2.813e+01 | 
| 05-08 20:17:36 epoch: 267| train loss i: [ 6.68	10.74	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.64	10.8 	 5.52	 2.93	 1.76	 0.47] | 
| 05-08 20:19:08 epoch: 268| time: 91.9s| train loss: +2.810e+01 | test loss: +2.834e+01 | 
| 05-08 20:19:08 epoch: 268| train loss i: [ 6.65	10.74	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.79	10.77	 5.56	 2.95	 1.78	 0.48] | 
| 05-08 20:20:39 epoch: 269| time: 91.5s| train loss: +2.810e+01 | test loss: +2.811e+01 | 
| 05-08 20:20:39 epoch: 269| train loss i: [ 6.65	10.73	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.63	10.78	 5.54	 2.93	 1.77	 0.47] | 
| 05-08 20:22:10 epoch: 270| time: 91.2s| train loss: +2.811e+01 | test loss: +2.810e+01 | 
| 05-08 20:22:10 epoch: 270| train loss i: [ 6.65	10.75	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.78	10.64	 5.5 	 2.94	 1.76	 0.47] | 
| 05-08 20:23:42 epoch: 271| time: 92.2s| train loss: +2.808e+01 | test loss: +2.822e+01 | 
| 05-08 20:23:42 epoch: 271| train loss i: [ 6.64	10.74	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.75	10.76	 5.53	 2.94	 1.77	 0.47] | 
| 05-08 20:25:15 epoch: 272| time: 92.2s| train loss: +2.808e+01 | test loss: +2.810e+01 | 
| 05-08 20:25:15 epoch: 272| train loss i: [ 6.64	10.73	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.74	10.71	 5.48	 2.94	 1.77	 0.47] | 
| 05-08 20:26:48 epoch: 273| time: 93.0s| train loss: +2.805e+01 | test loss: +2.832e+01 | 
| 05-08 20:26:48 epoch: 273| train loss i: [ 6.6 	10.74	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.9 	10.73	 5.51	 2.94	 1.77	 0.47] | 
| 05-08 20:28:19 epoch: 274| time: 91.6s| train loss: +2.810e+01 | test loss: +2.833e+01 | 
| 05-08 20:28:19 epoch: 274| train loss i: [ 6.67	10.72	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.81	10.8 	 5.52	 2.95	 1.77	 0.48] | 
| 05-08 20:29:51 epoch: 275| time: 91.4s| train loss: +2.805e+01 | test loss: +2.864e+01 | 
| 05-08 20:29:51 epoch: 275| train loss i: [ 6.61	10.74	 5.52	 2.94	 1.77	 0.47] test loss i: [ 7.1 	10.8 	 5.54	 2.96	 1.77	 0.47] | 
| 05-08 20:31:22 epoch: 276| time: 91.3s| train loss: +2.814e+01 | test loss: +2.816e+01 | 
| 05-08 20:31:22 epoch: 276| train loss i: [ 6.67	10.76	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.72	10.76	 5.5 	 2.94	 1.77	 0.47] | 
| 05-08 20:32:53 epoch: 277| time: 91.2s| train loss: +2.807e+01 | test loss: +2.816e+01 | 
| 05-08 20:32:53 epoch: 277| train loss i: [ 6.62	10.75	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.7 	10.78	 5.5 	 2.94	 1.77	 0.47] | 
| 05-08 20:34:25 epoch: 278| time: 91.9s| train loss: +2.807e+01 | test loss: +2.802e+01 | 
| 05-08 20:34:25 epoch: 278| train loss i: [ 6.64	10.73	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.57	10.75	 5.52	 2.95	 1.76	 0.47] | 
| 05-08 20:35:57 epoch: 279| time: 91.6s| train loss: +2.807e+01 | test loss: +2.801e+01 | 
| 05-08 20:35:57 epoch: 279| train loss i: [ 6.62	10.74	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.59	10.72	 5.52	 2.94	 1.77	 0.47] | 
| 05-08 20:37:29 epoch: 280| time: 92.3s| train loss: +2.807e+01 | test loss: +2.843e+01 | 
| 05-08 20:37:29 epoch: 280| train loss i: [ 6.64	10.73	 5.51	 2.94	 1.77	 0.47] test loss i: [ 6.85	10.87	 5.53	 2.94	 1.77	 0.47] | 
| 05-08 20:38:59 epoch: 281| time: 89.8s| train loss: +2.807e+01 | test loss: +2.818e+01 | 
| 05-08 20:38:59 epoch: 281| train loss i: [ 6.61	10.75	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.7 	10.77	 5.52	 2.95	 1.77	 0.47] | 
| 05-08 20:40:30 epoch: 282| time: 91.5s| train loss: +2.813e+01 | test loss: +2.827e+01 | 
| 05-08 20:40:30 epoch: 282| train loss i: [ 6.68	10.74	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.72	10.87	 5.5 	 2.94	 1.76	 0.48] | 
| 05-08 20:42:02 epoch: 283| time: 91.2s| train loss: +2.804e+01 | test loss: +2.815e+01 | 
| 05-08 20:42:02 epoch: 283| train loss i: [ 6.59	10.75	 5.51	 2.94	 1.77	 0.47] test loss i: [ 6.66	10.8 	 5.52	 2.94	 1.76	 0.47] | 
| 05-08 20:43:33 epoch: 284| time: 91.4s| train loss: +2.807e+01 | test loss: +2.835e+01 | 
| 05-08 20:43:33 epoch: 284| train loss i: [ 6.62	10.74	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.89	10.78	 5.51	 2.94	 1.76	 0.47] | 
| 05-08 20:45:04 epoch: 285| time: 91.4s| train loss: +2.805e+01 | test loss: +2.839e+01 | 
| 05-08 20:45:04 epoch: 285| train loss i: [ 6.63	10.72	 5.51	 2.94	 1.77	 0.47] test loss i: [ 6.88	10.82	 5.52	 2.94	 1.76	 0.47] | 
| 05-08 20:46:36 epoch: 286| time: 91.5s| train loss: +2.808e+01 | test loss: +2.812e+01 | 
| 05-08 20:46:36 epoch: 286| train loss i: [ 6.63	10.75	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.69	10.75	 5.5 	 2.94	 1.77	 0.47] | 
| 05-08 20:48:07 epoch: 287| time: 91.7s| train loss: +2.809e+01 | test loss: +2.821e+01 | 
| 05-08 20:48:07 epoch: 287| train loss i: [ 6.65	10.74	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.72	10.79	 5.54	 2.94	 1.76	 0.47] | 
| 05-08 20:49:39 epoch: 288| time: 92.0s| train loss: +2.805e+01 | test loss: +2.832e+01 | 
| 05-08 20:49:39 epoch: 288| train loss i: [ 6.64	10.7 	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.76	10.84	 5.53	 2.96	 1.77	 0.47] | 
| 05-08 20:51:11 epoch: 289| time: 91.4s| train loss: +2.808e+01 | test loss: +2.824e+01 | 
| 05-08 20:51:11 epoch: 289| train loss i: [ 6.63	10.75	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.7 	10.82	 5.51	 2.95	 1.78	 0.48] | 
| 05-08 20:52:42 epoch: 290| time: 91.4s| train loss: +2.803e+01 | test loss: +2.801e+01 | 
| 05-08 20:52:42 epoch: 290| train loss i: [ 6.61	10.72	 5.52	 2.95	 1.77	 0.47] test loss i: [ 6.58	10.75	 5.51	 2.94	 1.77	 0.47] | 
| 05-08 20:54:14 epoch: 291| time: 91.7s| train loss: +2.807e+01 | test loss: +2.810e+01 | 
| 05-08 20:54:14 epoch: 291| train loss i: [ 6.63	10.74	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.67	10.74	 5.51	 2.94	 1.77	 0.47] | 
| 05-08 20:55:45 epoch: 292| time: 91.1s| train loss: +2.808e+01 | test loss: +2.815e+01 | 
| 05-08 20:55:45 epoch: 292| train loss i: [ 6.64	10.74	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.75	10.7 	 5.53	 2.94	 1.77	 0.47] | 
| 05-08 20:57:21 epoch: 293| time: 95.5s| train loss: +2.807e+01 | test loss: +2.836e+01 | 
| 05-08 20:57:21 epoch: 293| train loss i: [ 6.63	10.74	 5.52	 2.94	 1.77	 0.47] test loss i: [ 6.86	10.76	 5.55	 2.94	 1.77	 0.48] | 
| 05-08 20:58:59 epoch: 294| time: 98.2s| train loss: +2.809e+01 | test loss: +2.823e+01 | 
| 05-08 20:58:59 epoch: 294| train loss i: [ 6.65	10.75	 5.51	 2.94	 1.77	 0.47] test loss i: [ 6.76	10.76	 5.51	 2.94	 1.77	 0.48] | 
| 05-08 21:00:37 epoch: 295| time: 97.8s| train loss: +2.804e+01 | test loss: +2.814e+01 | 
| 05-08 21:00:37 epoch: 295| train loss i: [ 6.62	10.73	 5.51	 2.94	 1.77	 0.47] test loss i: [ 6.75	10.71	 5.5 	 2.95	 1.77	 0.47] | 
| 05-08 21:02:14 epoch: 296| time: 97.9s| train loss: +2.806e+01 | test loss: +2.825e+01 | 
| 05-08 21:02:14 epoch: 296| train loss i: [ 6.6 	10.76	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.8 	10.73	 5.52	 2.94	 1.77	 0.47] | 
| 05-08 21:03:52 epoch: 297| time: 97.8s| train loss: +2.804e+01 | test loss: +2.826e+01 | 
| 05-08 21:03:52 epoch: 297| train loss i: [ 6.62	10.72	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.8 	10.77	 5.51	 2.94	 1.77	 0.47] | 
| 05-08 21:05:31 epoch: 298| time: 98.2s| train loss: +2.804e+01 | test loss: +2.829e+01 | 
| 05-08 21:05:31 epoch: 298| train loss i: [ 6.6 	10.74	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.81	10.77	 5.5 	 2.95	 1.77	 0.48] | 
| 05-08 21:07:09 epoch: 299| time: 98.2s| train loss: +2.804e+01 | test loss: +2.819e+01 | 
| 05-08 21:07:09 epoch: 299| train loss i: [ 6.63	10.72	 5.51	 2.95	 1.77	 0.47] test loss i: [ 6.72	10.78	 5.51	 2.95	 1.76	 0.47] | 
| 05-08 21:07:09 EXPERIMENT BEGIN: 
| 05-08 21:07:09 logging into exp/qm9/edp-gnn_QM9__May-08-13-30-52_3881078/sample/info.log
